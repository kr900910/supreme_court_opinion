{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Model (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import gensim\n",
    "import os, shutil, time, datetime\n",
    "from importlib import reload\n",
    "import pandas as pd\n",
    "from helpers import patched_numpy_io\n",
    "import model.NeuralBOW as NeuralBOW\n",
    "import model.NeuralBOW_withEmbeddings as NeuralBOW_withEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Load word2vec pre-trained model\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "word2vec_matrix = word2vec_model.syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add <UNK> and <PAD> vectors to word2vec matrix\n",
    "unk_vec = np.random.uniform(-0.25,0.25,300) \n",
    "pad_vec = np.zeros(300) \n",
    "\n",
    "word2vec_matrix = np.vstack((word2vec_matrix, unk_vec))\n",
    "word2vec_matrix = np.vstack((word2vec_matrix, pad_vec))\n",
    "\n",
    "word2vec_words = [k for k in word2vec_model.vocab.keys()]\n",
    "word2vec_words = word2vec_words + ['<unk>', '<pad>']\n",
    "\n",
    "id_to_word = dict(enumerate(word2vec_words))\n",
    "word_to_id = {v:k for k,v in id_to_word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_generator(data, max_word):\n",
    "    \n",
    "    # This function takes the input data in tokens and converted into matrix with indices\n",
    "    \n",
    "    inputs_x = np.zeros([len(data), max_word])\n",
    "    inputs_ns = np.zeros([len(data)])\n",
    "    inputs_y = np.zeros([len(data)])\n",
    "    num_doc = 0\n",
    "    \n",
    "    for i in data.index:\n",
    "        inputs_y[num_doc] = 1 if data.loc[i, 'y'] == 1 else 0\n",
    "        \n",
    "        # loop through words up to max words (clips longer docs)\n",
    "        words = data.loc[i, 'text']\n",
    "        num_word = 0\n",
    "        for w in words:\n",
    "            if num_word < max_word:\n",
    "                inputs_x[num_doc][num_word] = word_to_id.get(w)\n",
    "                num_word += 1\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        inputs_ns[num_doc]=num_word\n",
    "        \n",
    "        num_doc += 1\n",
    "    \n",
    "    return inputs_x, inputs_ns, inputs_y\n",
    "\n",
    "def batch_generator(inputs_x, inputs_ns, inputs_y, batch_size):\n",
    "    num_batch = (len(inputs_y)) // batch_size\n",
    "    for i in range(num_batch):\n",
    "        yield inputs_x[i*batch_size:(i+1)*batch_size], inputs_ns[i*batch_size:(i+1)*batch_size], inputs_y[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "def calc_acc(pred_y, true_y):\n",
    "    return np.sum(pred_y==true_y) / len(true_y)\n",
    "        \n",
    "def pretty_timedelta(fmt=\"%d:%02d:%02d\", since=None, until=None):\n",
    "    \"\"\"Pretty-print a timedelta, using the given format string.\"\"\"\n",
    "    since = since or time.time()\n",
    "    until = until or time.time()\n",
    "    delta_s = until - since\n",
    "    hours, remainder = divmod(delta_s, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return fmt % (hours, minutes, seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data = pickle.load(open(\"train.p\", \"rb\"))\n",
    "dev_data = pickle.load(open(\"dev.p\", \"rb\"))\n",
    "test_data = pickle.load(open(\"test.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the maximum word length per doc \n",
    "max_doc_length = 3000\n",
    "\n",
    "train_x, train_ns, train_y = input_generator(train_data, max_doc_length)\n",
    "dev_x, dev_ns, dev_y = input_generator(dev_data, max_doc_length)\n",
    "test_x, test_ns, test_y = input_generator(test_data, max_doc_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_x.shape)\n",
    "print(train_ns.shape)\n",
    "print(train_y.shape)\n",
    "print(dev_x.shape)\n",
    "print(dev_ns.shape)\n",
    "print(dev_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_ns.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate Model Without Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(NeuralBOW)\n",
    "\n",
    "x, ns, y = train_x, train_ns, train_y\n",
    "batch_size = 196 # Note the number of examples must be evenly divisible by this number\n",
    "num_epochs = 25\n",
    "\n",
    "# Specify model hyperparameters as used by model_fn\n",
    "model_params = dict(V=word2vec_matrix.shape[0], embed_dim=word2vec_matrix.shape[1], hidden_dims=[100,50],\n",
    "                    num_classes=2,\n",
    "                    encoder_type='bow', dropout_rate=0.25,\n",
    "                    lr=0.3, optimizer='adagrad', beta=0.01)\n",
    "model_fn = NeuralBOW.classifier_model_fn\n",
    "\n",
    "total_batches = 0\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    ##\n",
    "    # Construct the graph here. No session.run calls - just wiring up Tensors.\n",
    "    ##\n",
    "    # Add placeholders so we can feed in data.\n",
    "    x_ph_  = tf.placeholder(tf.int32, shape=[None, x.shape[1]])  # [batch_size, max_len]\n",
    "    ns_ph_ = tf.placeholder(tf.int32, shape=[None])              # [batch_size]\n",
    "    y_ph_  = tf.placeholder(tf.int32, shape=[None])              # [batch_size]\n",
    "    \n",
    "    # Construct the graph using model_fn\n",
    "    features = {\"ids\": x_ph_, \"ns\": ns_ph_}  # note that values are Tensors\n",
    "    estimator_spec = model_fn(features, labels=y_ph_, mode=tf.estimator.ModeKeys.TRAIN,\n",
    "                              params=model_params)\n",
    "    loss_     = estimator_spec.loss\n",
    "    train_op_ = estimator_spec.train_op\n",
    "    predictions_     = estimator_spec.predictions\n",
    "    \n",
    "    ##\n",
    "    # Done constructing the graph, now we can make session.run calls.\n",
    "    ##\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Train the graph\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        total_loss = 0\n",
    "        train_predict_prob = np.zeros(batch_size)\n",
    "        train_predict_class = np.zeros(batch_size)\n",
    "        batch_num = 0\n",
    "        \n",
    "        print(\"[epoch {:d}] Starting epoch {:d}\".format(epoch, epoch))\n",
    "\n",
    "        # Run a training epoch.\n",
    "        for (bx, bns, by) in batch_generator(x, ns, y, batch_size):\n",
    "            # feed NumPy arrays into the placeholder Tensors\n",
    "            feed_dict = {x_ph_: bx, ns_ph_: bns, y_ph_: by}\n",
    "            batch_loss, predictions, _ = sess.run([loss_, predictions_, train_op_], feed_dict=feed_dict)\n",
    "        \n",
    "            # Compute some statistics\n",
    "            total_batches += 1\n",
    "            total_loss += batch_loss * len(bx)  # re-scale, since batch loss is mean\n",
    "            \n",
    "            if batch_num == 0:\n",
    "                train_predict_prob = predictions['proba']\n",
    "                train_predict_class = predictions['max']\n",
    "            else:\n",
    "                train_predict_prob = np.concatenate((train_predict_prob, predictions['proba']))\n",
    "                train_predict_class = np.concatenate((train_predict_class, predictions['max']))\n",
    "            \n",
    "            batch_num += 1\n",
    "        \n",
    "        print(\"[epoch {:d}] Completed in {:s}\".format(epoch, pretty_timedelta(since=t0_epoch)))\n",
    "        print(\"Total loss for epoch: %0.2f\" % total_loss)\n",
    "        print(\"Accuracy on training data is: %0.2f\" % calc_acc(train_predict_class, train_y))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Generate accuracy and predicted probabilities for the test set\n",
    "    # Note that this runs the training step, but only updates after it has already made predictions\n",
    "    # Doing this because re-instantiating this with PREDICT mode later generates a new graph\n",
    "    \n",
    "    x, ns, y = test_x, test_ns, test_y\n",
    "    test_predict_prob = np.zeros(batch_size)\n",
    "    test_predict_class = np.zeros(batch_size)\n",
    "    batch_num = 0\n",
    "    \n",
    "    for (bx, bns, by) in batch_generator(x, ns, y, 237):\n",
    "        # feed NumPy arrays into the placeholder Tensors\n",
    "        feed_dict = {x_ph_: bx, ns_ph_: bns, y_ph_: by}\n",
    "        batch_loss, predictions, _ = sess.run([loss_, predictions_, train_op_], feed_dict=feed_dict)\n",
    "        \n",
    "        if batch_num == 0:\n",
    "            test_predict_prob = predictions['proba']\n",
    "            test_predict_class = predictions['max']\n",
    "        else:\n",
    "            test_predict_prob = np.concatenate((test_predict_prob, predictions['proba']))\n",
    "            test_predict_class = np.concatenate((test_predict_class, predictions['max']))\n",
    "            \n",
    "        batch_num += 1\n",
    "    print(\"Accuracy on testing data is: %0.2f\" % calc_acc(test_predict_class, test_y))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cross_entropy_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(1791.774, 1814.6]</th>\n",
       "      <td>1.061668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1814.6, 1837.2]</th>\n",
       "      <td>1.015342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1837.2, 1859.8]</th>\n",
       "      <td>1.049687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1859.8, 1882.4]</th>\n",
       "      <td>1.074655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1882.4, 1905.0]</th>\n",
       "      <td>1.050141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1905.0, 1927.6]</th>\n",
       "      <td>1.068226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1927.6, 1950.2]</th>\n",
       "      <td>1.005111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1950.2, 1972.8]</th>\n",
       "      <td>0.917637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1972.8, 1995.4]</th>\n",
       "      <td>0.821081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1995.4, 2018.0]</th>\n",
       "      <td>0.845767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    cross_entropy_loss\n",
       "(1791.774, 1814.6]            1.061668\n",
       "(1814.6, 1837.2]              1.015342\n",
       "(1837.2, 1859.8]              1.049687\n",
       "(1859.8, 1882.4]              1.074655\n",
       "(1882.4, 1905.0]              1.050141\n",
       "(1905.0, 1927.6]              1.068226\n",
       "(1927.6, 1950.2]              1.005111\n",
       "(1950.2, 1972.8]              0.917637\n",
       "(1972.8, 1995.4]              0.821081\n",
       "(1995.4, 2018.0]              0.845767"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, _ = pd.cut([int(x) for x in train_data.year_filed], 10, retbins=True)\n",
    "bins = pd.cut([int(x) for x in train_data.year_filed], 10, labels=False)\n",
    "\n",
    "preds_df = pd.DataFrame({'bins': bins, 'log_prob': np.log(train_predict_prob)})\n",
    "\n",
    "agg_df = preds_df.groupby('bins').agg(lambda x: -np.mean(x.log_prob))\n",
    "agg_df.index = out.categories\n",
    "agg_df.columns = ['cross_entropy_loss']\n",
    "\n",
    "agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Train Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: consistently getting an error ''Tensor' object has no attribute 'sparse_read'. Possibly due to not writing a vocab?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tf_bow_20180810-0621', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fbe0e7db4e0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "\n",
      "To view training (once it starts), run:\n",
      "\n",
      "    tensorboard --logdir='/tmp/tf_bow_20180810-0621' --port 6006\n",
      "\n",
      "Then in your browser, open: http://localhost:6006\n"
     ]
    }
   ],
   "source": [
    "reload(NeuralBOW)\n",
    "\n",
    "# Specify model hyperparameters as used by model_fn\n",
    "model_params = dict(V=word2vec_matrix.shape[0], embed_dim=word2vec_matrix.shape[1], hidden_dims=[25],\n",
    "                    num_classes=2,\n",
    "                    encoder_type='bow', dropout_rate=0.25,\n",
    "                    lr=0.1, optimizer='adagrad', beta=0.01)\n",
    "\n",
    "checkpoint_dir = \"/tmp/tf_bow_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "if os.path.isdir(checkpoint_dir):\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "    \n",
    "# Write vocabulary to file, so TensorBoard can label embeddings.\n",
    "# creates checkpoint_dir/projector_config.pbtxt and checkpoint_dir/metadata.tsv\n",
    "#ds.vocab.write_projector_config(checkpoint_dir, \"Encoder/Embedding_Layer/W_embed\")\n",
    "\n",
    "model = tf.estimator.Estimator(model_fn=NeuralBOW.classifier_model_fn, \n",
    "                               params=model_params,\n",
    "                               model_dir=checkpoint_dir)\n",
    "print(\"\")\n",
    "print(\"To view training (once it starts), run:\\n\")\n",
    "print(\"    tensorboard --logdir='{:s}' --port 6006\".format(checkpoint_dir))\n",
    "print(\"\\nThen in your browser, open: http://localhost:6006\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Value passed to parameter 'indices' has DataType float64 not in list of allowed values: int32, int64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   2733\u001b[0m     \u001b[0;31m# introducing a circular dependency.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2734\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2735\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'sparse_read'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-3c597ad6458b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total_epochs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mtrain_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eval_every'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Train for a few epochs, then evaluate on dev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0meval_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev_input_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dev\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m    841\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m       estimator_spec = self._call_model_fn(\n\u001b[0;32m--> 856\u001b[0;31m           features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\n\u001b[0m\u001b[1;32m    857\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n\u001b[1;32m    858\u001b[0m                                              \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/final/model/NeuralBOW.py\u001b[0m in \u001b[0;36mclassifier_model_fn\u001b[0;34m(features, labels, mode, params)\u001b[0m\n\u001b[1;32m    172\u001b[0m             h_, xs_ = BOW_encoder(features['ids'], features['ns'],\n\u001b[1;32m    173\u001b[0m                                   \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m                                   **params)\n\u001b[0m\u001b[1;32m    175\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         raise ValueError(\"Error: unsupported encoder type \"\n",
      "\u001b[0;32m~/final/model/NeuralBOW.py\u001b[0m in \u001b[0;36mBOW_encoder\u001b[0;34m(ids_, ns_, V, embed_dim, hidden_dims, dropout_rate, is_training, **unused_kw)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Embedding_Layer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;31m#### YOUR CODE HERE ####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mxs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# replace with a call to embedding_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;31m#### END(YOUR CODE) ####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/final/model/NeuralBOW.py\u001b[0m in \u001b[0;36membedding_layer\u001b[0;34m(ids_, V, embed_dim, init_scale)\u001b[0m\n\u001b[1;32m     26\u001b[0m                                \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_uniform_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minit_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                                dtype=tf.float32)\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mxs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_embed_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m#### END(YOUR CODE) ####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py\u001b[0m in \u001b[0;36membedding_lookup\u001b[0;34m(params, ids, partition_strategy, name, validate_indices, max_norm)\u001b[0m\n\u001b[1;32m    306\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m       \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m       transform_fn=None)\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py\u001b[0m in \u001b[0;36m_embedding_lookup_and_transform\u001b[0;34m(params, ids, partition_strategy, name, max_norm, transform_fn)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mtransform_fn\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         result = _clip(array_ops.gather(params[0], ids, name=name),\n\u001b[0m\u001b[1;32m    132\u001b[0m                        ids, max_norm)\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtransform_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   2734\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2735\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2736\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mgather_v2\u001b[0;34m(params, indices, axis, name)\u001b[0m\n\u001b[1;32m   3063\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3064\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 3065\u001b[0;31m         \"GatherV2\", params=params, indices=indices, axis=axis, name=name)\n\u001b[0m\u001b[1;32m   3066\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    607\u001b[0m               _SatisfiesTypeConstraint(base_type,\n\u001b[1;32m    608\u001b[0m                                        \u001b[0m_Attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m                                        param_name=input_name)\n\u001b[0m\u001b[1;32m    610\u001b[0m             \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0minferred_from\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[0;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m           \u001b[0;34m\"allowed values: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           (param_name, dtypes.as_dtype(dtype).name,\n\u001b[0;32m---> 60\u001b[0;31m            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Value passed to parameter 'indices' has DataType float64 not in list of allowed values: int32, int64"
     ]
    }
   ],
   "source": [
    "# Training params, just used in this cell for the input_fn-s\n",
    "train_params = dict(batch_size=196, total_epochs=6, eval_every=2)\n",
    "assert(train_params['total_epochs'] % train_params['eval_every'] == 0)\n",
    "\n",
    "# Construct and train the model, saving checkpoints to the directory above.\n",
    "# Input function for training set batches\n",
    "# Do 'eval_every' epochs at once, followed by evaluating on the dev set.\n",
    "# NOTE: use patch_numpy_io.numpy_input_fn instead of tf.estimator.inputs.numpy_input_fn\n",
    "train_input_fn = patched_numpy_io.numpy_input_fn(\n",
    "                    x={\"ids\": train_x, \"ns\": train_ns}, y=train_y,\n",
    "                    batch_size=train_params['batch_size'], \n",
    "                    num_epochs=train_params['eval_every'], shuffle=True, seed=42\n",
    "                 )\n",
    "\n",
    "# Input function for dev set batches. As above, but:\n",
    "# - Don't randomize order\n",
    "# - Iterate exactly once (one epoch)\n",
    "dev_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": dev_x, \"ns\": dev_ns}, y=dev_y,\n",
    "                    batch_size=196, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "for _ in range(train_params['total_epochs'] // train_params['eval_every']):\n",
    "    # Train for a few epochs, then evaluate on dev\n",
    "    model.train(input_fn=train_input_fn)\n",
    "    eval_metrics = model.evaluate(input_fn=dev_input_fn, name=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate Model with word2vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'Encoder/Embedding_Layer/Placeholder' with dtype float and shape [3000002,300]\n\t [[Node: Encoder/Embedding_Layer/Placeholder = Placeholder[dtype=DT_FLOAT, shape=[3000002,300], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'Encoder/Embedding_Layer/Placeholder', defined at:\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-133-4e9b41807a3c>\", line 28, in <module>\n    params=model_params)\n  File \"/home/alyssa_yerukhimov/final/model/NeuralBOW_withEmbeddings.py\", line 178, in classifier_model_fn\n    **params)\n  File \"/home/alyssa_yerukhimov/final/model/NeuralBOW_withEmbeddings.py\", line 147, in BOW_encoder\n    xs_ = embedding_layer(ids_, V, embed_dim)  # replace with a call to embedding_layer\n  File \"/home/alyssa_yerukhimov/final/model/NeuralBOW_withEmbeddings.py\", line 25, in embedding_layer\n    embedding_ = tf.placeholder(tf.float32, shape=(V, embed_dim))\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1808, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 4848, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Encoder/Embedding_Layer/Placeholder' with dtype float and shape [3000002,300]\n\t [[Node: Encoder/Embedding_Layer/Placeholder = Placeholder[dtype=DT_FLOAT, shape=[3000002,300], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Encoder/Embedding_Layer/Placeholder' with dtype float and shape [3000002,300]\n\t [[Node: Encoder/Embedding_Layer/Placeholder = Placeholder[dtype=DT_FLOAT, shape=[3000002,300], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-4e9b41807a3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Done constructing the graph, now we can make session.run calls.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0membedding_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mword2vec_matrix\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# Train the graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Encoder/Embedding_Layer/Placeholder' with dtype float and shape [3000002,300]\n\t [[Node: Encoder/Embedding_Layer/Placeholder = Placeholder[dtype=DT_FLOAT, shape=[3000002,300], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'Encoder/Embedding_Layer/Placeholder', defined at:\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-133-4e9b41807a3c>\", line 28, in <module>\n    params=model_params)\n  File \"/home/alyssa_yerukhimov/final/model/NeuralBOW_withEmbeddings.py\", line 178, in classifier_model_fn\n    **params)\n  File \"/home/alyssa_yerukhimov/final/model/NeuralBOW_withEmbeddings.py\", line 147, in BOW_encoder\n    xs_ = embedding_layer(ids_, V, embed_dim)  # replace with a call to embedding_layer\n  File \"/home/alyssa_yerukhimov/final/model/NeuralBOW_withEmbeddings.py\", line 25, in embedding_layer\n    embedding_ = tf.placeholder(tf.float32, shape=(V, embed_dim))\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1808, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 4848, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/alyssa_yerukhimov/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Encoder/Embedding_Layer/Placeholder' with dtype float and shape [3000002,300]\n\t [[Node: Encoder/Embedding_Layer/Placeholder = Placeholder[dtype=DT_FLOAT, shape=[3000002,300], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "reload(NeuralBOW_withEmbeddings)\n",
    "\n",
    "x, ns, y = train_x, train_ns, train_y\n",
    "batch_size = 196 # Note the number of examples must be evenly divisible by this number\n",
    "num_epochs = 25\n",
    "\n",
    "# Specify model hyperparameters as used by model_fn\n",
    "model_params = dict(V=word2vec_matrix.shape[0], embed_dim=word2vec_matrix.shape[1], hidden_dims=[100,50],\n",
    "                    num_classes=2,\n",
    "                    encoder_type='bow', dropout_rate=0.25,\n",
    "                    lr=0.1, optimizer='adagrad', beta=0.01)\n",
    "model_fn = NeuralBOW_withEmbeddings.classifier_model_fn\n",
    "\n",
    "total_batches = 0\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    ##\n",
    "    # Construct the graph here. No session.run calls - just wiring up Tensors.\n",
    "    ##\n",
    "    # Add placeholders so we can feed in data.\n",
    "    x_ph_  = tf.placeholder(tf.int32, shape=[None, x.shape[1]])  # [batch_size, max_len]\n",
    "    ns_ph_ = tf.placeholder(tf.int32, shape=[None])              # [batch_size]\n",
    "    y_ph_  = tf.placeholder(tf.int32, shape=[None])              # [batch_size]\n",
    "    embedding_  = tf.placeholder(tf.float32, shape=(word2vec_matrix.shape[0], word2vec_matrix.shape[1])) # [V, embed_dim]\n",
    "    \n",
    "    # Construct the graph using model_fn\n",
    "    features = {\"ids\": x_ph_, \"ns\": ns_ph_}  # note that values are Tensors\n",
    "    estimator_spec = model_fn(features, labels=y_ph_, mode=tf.estimator.ModeKeys.TRAIN,\n",
    "                              params=model_params)\n",
    "    loss_     = estimator_spec.loss\n",
    "    train_op_ = estimator_spec.train_op\n",
    "    predictions_     = estimator_spec.predictions\n",
    "    \n",
    "    ##\n",
    "    # Done constructing the graph, now we can make session.run calls.\n",
    "    ##\n",
    "    sess.run(tf.global_variables_initializer(), feed_dict={embedding_: word2vec_matrix})\n",
    "    \n",
    "    # Train the graph\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        total_loss = 0\n",
    "        train_predict_prob = np.zeros(batch_size)\n",
    "        train_predict_class = np.zeros(batch_size)\n",
    "        batch_num = 0\n",
    "        \n",
    "        print(\"[epoch {:d}] Starting epoch {:d}\".format(epoch, epoch))\n",
    "\n",
    "        # Run a training epoch.\n",
    "        for (bx, bns, by) in batch_generator(x, ns, y, batch_size):\n",
    "            # feed NumPy arrays into the placeholder Tensors\n",
    "            feed_dict = {x_ph_: bx, ns_ph_: bns, y_ph_: by}\n",
    "            batch_loss, predictions, _ = sess.run([loss_, predictions_, train_op_], feed_dict=feed_dict)\n",
    "        \n",
    "            # Compute some statistics\n",
    "            total_batches += 1\n",
    "            total_loss += batch_loss * len(bx)  # re-scale, since batch loss is mean\n",
    "            \n",
    "            if batch_num == 0:\n",
    "                train_predict_prob = predictions['proba']\n",
    "                train_predict_class = predictions['max']\n",
    "            else:\n",
    "                train_predict_prob = np.concatenate((train_predict_prob, predictions['proba']))\n",
    "                train_predict_class = np.concatenate((train_predict_class, predictions['max']))\n",
    "            \n",
    "            batch_num += 1\n",
    "        \n",
    "        print(\"[epoch {:d}] Completed in {:s}\".format(epoch, pretty_timedelta(since=t0_epoch)))\n",
    "        print(\"Total loss for epoch: %0.2f\" % total_loss)\n",
    "        print(\"Accuracy on training data is: %0.2f\" % calc_acc(train_predict_class, train_y))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Generate accuracy and predicted probabilities for the test set\n",
    "    # Note that this runs the training step, but only updates after it has already made predictions\n",
    "    # Doing this because re-instantiating this with PREDICT mode later generates a new graph\n",
    "    \n",
    "    x, ns, y = test_x, test_ns, test_y\n",
    "    test_predict_prob = np.zeros(batch_size)\n",
    "    test_predict_class = np.zeros(batch_size)\n",
    "    batch_num = 0\n",
    "    \n",
    "    for (bx, bns, by) in batch_generator(x, ns, y, 237):\n",
    "        # feed NumPy arrays into the placeholder Tensors\n",
    "        feed_dict = {x_ph_: bx, ns_ph_: bns, y_ph_: by}\n",
    "        batch_loss, predictions, _ = sess.run([loss_, predictions_, train_op_], feed_dict=feed_dict)\n",
    "        \n",
    "        if batch_num == 0:\n",
    "            test_predict_prob = predictions['proba']\n",
    "            test_predict_class = predictions['max']\n",
    "        else:\n",
    "            test_predict_prob = np.concatenate((test_predict_prob, predictions['proba']))\n",
    "            test_predict_class = np.concatenate((test_predict_class, predictions['max']))\n",
    "            \n",
    "        batch_num += 1\n",
    "    print(\"Accuracy on testing data is: %0.2f\" % calc_acc(test_predict_class, test_y))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cross_entropy_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(1791.774, 1814.6]</th>\n",
       "      <td>1.061668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1814.6, 1837.2]</th>\n",
       "      <td>1.015342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1837.2, 1859.8]</th>\n",
       "      <td>1.049687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1859.8, 1882.4]</th>\n",
       "      <td>1.074655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1882.4, 1905.0]</th>\n",
       "      <td>1.050141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1905.0, 1927.6]</th>\n",
       "      <td>1.068226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1927.6, 1950.2]</th>\n",
       "      <td>1.005111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1950.2, 1972.8]</th>\n",
       "      <td>0.917637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1972.8, 1995.4]</th>\n",
       "      <td>0.821081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1995.4, 2018.0]</th>\n",
       "      <td>0.845767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    cross_entropy_loss\n",
       "(1791.774, 1814.6]            1.061668\n",
       "(1814.6, 1837.2]              1.015342\n",
       "(1837.2, 1859.8]              1.049687\n",
       "(1859.8, 1882.4]              1.074655\n",
       "(1882.4, 1905.0]              1.050141\n",
       "(1905.0, 1927.6]              1.068226\n",
       "(1927.6, 1950.2]              1.005111\n",
       "(1950.2, 1972.8]              0.917637\n",
       "(1972.8, 1995.4]              0.821081\n",
       "(1995.4, 2018.0]              0.845767"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, _ = pd.cut([int(x) for x in train_data.year_filed], 10, retbins=True)\n",
    "bins = pd.cut([int(x) for x in train_data.year_filed], 10, labels=False)\n",
    "\n",
    "preds_df = pd.DataFrame({'bins': bins, 'log_prob': np.log(train_predict_prob)})\n",
    "\n",
    "agg_df = preds_df.groupby('bins').agg(lambda x: -np.mean(x.log_prob))\n",
    "agg_df.index = out.categories\n",
    "agg_df.columns = ['cross_entropy_loss']\n",
    "\n",
    "agg_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
