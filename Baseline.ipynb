{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from helpers import vocabulary\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and vocabulary preprocessed\n",
    "data = pickle.load(open(\"./data/data.p\", \"rb\"))\n",
    "vocab = pickle.load(open(\"./vocab.p\", \"rb\"))\n",
    "vocab_12314 = pickle.load(open(\"./vocab_12314.p\", \"rb\"))\n",
    "vocab_6155 = pickle.load(open(\"./vocab_6155.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional pre-processing of data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out other political party\n",
    "df = data[(data.political_party=='r')|(data.political_party=='d')]\n",
    "\n",
    "# Shuffle data\n",
    "df = df.sample(frac=1, random_state=38291)\n",
    "df.loc[:,'y'] = [1 if p == 'd' else 0 for p in df.political_party]\n",
    "\n",
    "# Divide data into train/dev/test\n",
    "train_number = int(len(df)*0.8)\n",
    "dev_number = int(len(df)*0.1)\n",
    "\n",
    "train_data = df[0:train_number]\n",
    "dev_data = df[train_number:train_number+dev_number]\n",
    "test_data = df[train_number+dev_number:]\n",
    "\n",
    "pickle.dump(train_data, open(\"train.p\", \"wb\"))\n",
    "pickle.dump(dev_data, open(\"dev.p\", \"wb\"))\n",
    "pickle.dump(test_data, open(\"test.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out data points pre-1935 because Dem / Rep platforms switched meaning around that time\n",
    "df.year_filed = [int(x) for x in df.year_filed]\n",
    "df_post1935 = df[(df.year_filed>int(1935))]\n",
    "\n",
    "# Shuffle data\n",
    "df_post1935 = df_post1935.sample(frac=1, random_state=38291)\n",
    "df_post1935.loc[:,'y'] = [1 if p == 'd' else 0 for p in df_post1935.political_party]\n",
    "\n",
    "# Divide data into train/dev/test\n",
    "train_number_post1935 = int(len(df_post1935)*0.8)\n",
    "dev_number_post1935 = int(len(df_post1935)*0.1)\n",
    "\n",
    "train_data_post1935 = df_post1935[0:train_number_post1935]\n",
    "dev_data_post1935 = df_post1935[train_number_post1935:train_number_post1935+dev_number_post1935]\n",
    "test_data_post1935 = df_post1935[train_number_post1935+dev_number_post1935:]\n",
    "\n",
    "pickle.dump(train_data_post1935, open(\"train_post1935.p\", \"wb\"))\n",
    "pickle.dump(dev_data_post1935, open(\"dev_post1935.p\", \"wb\"))\n",
    "pickle.dump(test_data_post1935, open(\"test_post1935.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22232, 6)\n",
      "(21316, 7)\n",
      "(7391, 7)\n"
     ]
    }
   ],
   "source": [
    "# Double check df sizes make sense\n",
    "print(data.shape)\n",
    "print(df.shape)\n",
    "print(df_post1935.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most Common Class Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5773 Democrat examples, and 11279 Republican examples\n",
      "Training set accuracy is: 0.66\n",
      "Test set accuracy is: 0.65\n"
     ]
    }
   ],
   "source": [
    "# actual values\n",
    "train_y = train_data.y\n",
    "dev_y = dev_data.y\n",
    "test_y = test_data.y\n",
    "\n",
    "# predicted values\n",
    "print(\"There are %d Democrat examples, and %d Republican examples\" % (sum(train_y==1), sum(train_y==0)))\n",
    "train_pred = np.zeros(len(train_y))\n",
    "test_pred = np.zeros(len(test_y))\n",
    "\n",
    "# calculate accuracy\n",
    "print(\"Training set accuracy is: %.2f\" % metrics.accuracy_score(train_y, train_pred))\n",
    "print(\"Test set accuracy is: %.2f\" % metrics.accuracy_score(test_y, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Regression Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define TF-IDF vectorizer; Using top 1000 words as stop words.\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None,\n",
    "    vocabulary=vocab.word_to_id,\n",
    "    stop_words=[w for w, i in vocab.unigram_counts.most_common(1000)])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to TFIDF data sets\n",
    "\n",
    "train_x = tfidf.fit_transform(train_data.text)\n",
    "train_y = train_data.y\n",
    "\n",
    "dev_x = tfidf.transform(dev_data.text)\n",
    "dev_y = dev_data.y\n",
    "\n",
    "test_x = tfidf.transform(test_data.text)\n",
    "test_y = test_data.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17052, 10000)\n",
      "(17052,)\n",
      "(2131, 10000)\n",
      "(2131,)\n",
      "(2133, 10000)\n",
      "(2133,)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(dev_x.shape)\n",
    "print(dev_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.800903119869\n",
      "0.76208352886\n"
     ]
    }
   ],
   "source": [
    "# Fit logitic regression (L1)\n",
    "clf_l1 = linear_model.LogisticRegression(penalty='l1')\n",
    "clf_l1.fit(train_x, train_y)\n",
    "\n",
    "print(clf_l1.score(train_x, train_y))\n",
    "print(clf_l1.score(dev_x, dev_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.814273985456\n",
      "0.741905208822\n"
     ]
    }
   ],
   "source": [
    "# Fit logitic regression (L2)\n",
    "clf_l2 = linear_model.LogisticRegression(penalty='l2')\n",
    "clf_l2.fit(train_x, train_y)\n",
    "\n",
    "print(clf_l2.score(train_x, train_y))\n",
    "print(clf_l2.score(dev_x, dev_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.666490734225\n",
      "0.668230877522\n",
      "\n",
      "0.2\n",
      "0.693642974431\n",
      "0.69544814641\n",
      "\n",
      "0.3\n",
      "0.712643678161\n",
      "0.709995307367\n",
      "\n",
      "0.4\n",
      "0.73099929627\n",
      "0.719849835758\n",
      "\n",
      "0.5\n",
      "0.747654234107\n",
      "0.732989206945\n",
      "\n",
      "0.6\n",
      "0.761435608726\n",
      "0.741435945565\n",
      "\n",
      "0.7\n",
      "0.773926812104\n",
      "0.752229000469\n",
      "\n",
      "0.8\n",
      "0.78471733521\n",
      "0.755983106523\n",
      "\n",
      "0.9\n",
      "0.792634295097\n",
      "0.76208352886\n",
      "\n",
      "1.0\n",
      "0.800903119869\n",
      "0.76208352886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try different hyperparameters\n",
    "for c in np.arange(0.1, 1.1, 0.1):\n",
    "    clf_l1 = linear_model.LogisticRegression(penalty='l1', C=c)\n",
    "    clf_l1.fit(train_x, train_y)\n",
    "\n",
    "    print(c)\n",
    "    print(clf_l1.score(train_x, train_y))\n",
    "    print(clf_l1.score(dev_x, dev_y))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.800903119869\n",
      "0.746835443038\n"
     ]
    }
   ],
   "source": [
    "# Final model with L1 regularization with C = 1.0\n",
    "clf_l1 = linear_model.LogisticRegression(penalty='l1', C=1.0)\n",
    "clf_l1.fit(train_x, train_y)\n",
    "\n",
    "print(clf_l1.score(train_x, train_y))\n",
    "print(clf_l1.score(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Democrat Top 10:\n",
      "1 - frankfurter - 32.41\n",
      "2 - fuller - 30.75\n",
      "3 - whereof - 22.49\n",
      "4 - douglas - 16.06\n",
      "5 - problem - 14.94\n",
      "6 - compare - 13.75\n",
      "7 - furthermore - 12.84\n",
      "8 - insofar - 12.26\n",
      "9 - consequently - 9.25\n",
      "10 - remarked - 9.21\n",
      "\n",
      "Republican Top 10:\n",
      "1 - brennan - -24.81\n",
      "2 - holmes - -21.61\n",
      "3 - waite - -19.02\n",
      "4 - sutherland - -18.17\n",
      "5 - brewer - -11.85\n",
      "6 - observed - -9.51\n",
      "7 - isso - -9.27\n",
      "8 - pursuance - -8.9\n",
      "9 - stevens - -8.29\n",
      "10 - besides - -8.0\n"
     ]
    }
   ],
   "source": [
    "# Find top 10 words for both Democrats and Republicans\n",
    "coeff = clf_l1.coef_.flatten()\n",
    "democrat_top_10 = list(sorted(range(len(coeff)), key=lambda i: coeff[i], reverse=True)[0:10])\n",
    "republican_top_10 = list(sorted(range(len(coeff)), key=lambda i: coeff[i])[0:10])\n",
    "\n",
    "print(\"Democrat Top 10:\")\n",
    "for i, d in enumerate(democrat_top_10):\n",
    "    print(i+1, '-', vocab.id_to_word.get(d), '-', round(coeff[d],2))\n",
    "\n",
    "print()\n",
    "print(\"Republican Top 10:\")\n",
    "for i, r in enumerate(republican_top_10):\n",
    "    print(i+1, '-', vocab.id_to_word.get(r), '-', round(coeff[r],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cross_entropy_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(1791.774, 1814.6]</th>\n",
       "      <td>1.169719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1814.6, 1837.2]</th>\n",
       "      <td>0.492416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1837.2, 1859.8]</th>\n",
       "      <td>0.527713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1859.8, 1882.4]</th>\n",
       "      <td>1.843899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1882.4, 1905.0]</th>\n",
       "      <td>1.652408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1905.0, 1927.6]</th>\n",
       "      <td>1.517391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1927.6, 1950.2]</th>\n",
       "      <td>0.974788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1950.2, 1972.8]</th>\n",
       "      <td>0.749886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1972.8, 1995.4]</th>\n",
       "      <td>1.305375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1995.4, 2018.0]</th>\n",
       "      <td>1.250749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    cross_entropy_loss\n",
       "(1791.774, 1814.6]            1.169719\n",
       "(1814.6, 1837.2]              0.492416\n",
       "(1837.2, 1859.8]              0.527713\n",
       "(1859.8, 1882.4]              1.843899\n",
       "(1882.4, 1905.0]              1.652408\n",
       "(1905.0, 1927.6]              1.517391\n",
       "(1927.6, 1950.2]              0.974788\n",
       "(1950.2, 1972.8]              0.749886\n",
       "(1972.8, 1995.4]              1.305375\n",
       "(1995.4, 2018.0]              1.250749"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, _ = pd.cut([int(x) for x in train_data.year_filed], 10, retbins=True)\n",
    "bins = pd.cut([int(x) for x in train_data.year_filed], 10, labels=False)\n",
    "\n",
    "preds = np.log(clf_l1.predict_proba(train_x)[:,1])\n",
    "preds_df = pd.DataFrame({'bins': bins, 'log_prob': preds})\n",
    "\n",
    "agg_df = preds_df.groupby('bins').agg(lambda x: -np.mean(x.log_prob))\n",
    "agg_df.index = out.categories\n",
    "agg_df.columns = ['cross_entropy_loss']\n",
    "\n",
    "agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: removing judge names did not lower accuracy by that much, but gave us a more meaningful parameter set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified Regression Parameters - exclude judges in stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17052, 10000)\n",
      "(17052,)\n",
      "(2131, 10000)\n",
      "(2131,)\n",
      "(2133, 10000)\n",
      "(2133,)\n"
     ]
    }
   ],
   "source": [
    "# Define TF-IDF vectorizer; Using top 1000 words as stop words.\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "# Remove judge names b/c these are strong indicators of political party without indicating how language is used in opinions\n",
    "stop_word_set = [w for w, i in vocab.unigram_counts.most_common(1000)]\n",
    "stop_word_set.extend([judge_name for judge_name in set(w.lower() for w in data.name_last)])\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None,\n",
    "    vocabulary=vocab.word_to_id,\n",
    "    stop_words=stop_word_set)  \n",
    "\n",
    "# Transform to TFIDF data sets\n",
    "\n",
    "train_x = tfidf.fit_transform(train_data.text)\n",
    "train_y = train_data.y\n",
    "\n",
    "dev_x = tfidf.transform(dev_data.text)\n",
    "dev_y = dev_data.y\n",
    "\n",
    "test_x = tfidf.transform(test_data.text)\n",
    "test_y = test_data.y\n",
    "\n",
    "# print shapes for confirmation\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(dev_x.shape)\n",
    "print(dev_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 stats:\n",
      "0.772695285011\n",
      "0.735804786485\n",
      "L2 stats:\n",
      "0.802076002815\n",
      "0.726419521351\n"
     ]
    }
   ],
   "source": [
    "# Fit logitic regression (L1)\n",
    "clf_l1 = linear_model.LogisticRegression(penalty='l1')\n",
    "clf_l1.fit(train_x, train_y)\n",
    "\n",
    "print(\"L1 stats:\")\n",
    "print(clf_l1.score(train_x, train_y))\n",
    "print(clf_l1.score(dev_x, dev_y))\n",
    "\n",
    "# Fit logitic regression (L2)\n",
    "clf_l2 = linear_model.LogisticRegression(penalty='l2')\n",
    "clf_l2.fit(train_x, train_y)\n",
    "\n",
    "print(\"L2 stats:\")\n",
    "print(clf_l2.score(train_x, train_y))\n",
    "print(clf_l2.score(dev_x, dev_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.661447337556\n",
      "0.660722665415\n",
      "\n",
      "0.2\n",
      "0.676577527563\n",
      "0.678085405913\n",
      "\n",
      "0.3\n",
      "0.690476190476\n",
      "0.690755513843\n",
      "\n",
      "0.4\n",
      "0.703377902885\n",
      "0.697325199437\n",
      "\n",
      "0.5\n",
      "0.718273516303\n",
      "0.702956358517\n",
      "\n",
      "0.6\n",
      "0.730764719681\n",
      "0.713749413421\n",
      "\n",
      "0.7\n",
      "0.742904058175\n",
      "0.717503519474\n",
      "\n",
      "0.8\n",
      "0.755277973258\n",
      "0.724073205068\n",
      "\n",
      "0.9\n",
      "0.764015951208\n",
      "0.732050680432\n",
      "\n",
      "1.0\n",
      "0.772695285011\n",
      "0.735804786485\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try different hyperparameters\n",
    "for c in np.arange(0.1, 1.1, 0.1):\n",
    "    clf_l1 = linear_model.LogisticRegression(penalty='l1', C=c)\n",
    "    clf_l1.fit(train_x, train_y)\n",
    "\n",
    "    print(c)\n",
    "    print(clf_l1.score(train_x, train_y))\n",
    "    print(clf_l1.score(dev_x, dev_y))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.772695285011\n",
      "0.719643694327\n"
     ]
    }
   ],
   "source": [
    "# Final model with L1 regularization with C = 1.0\n",
    "clf_l1 = linear_model.LogisticRegression(penalty='l1', C=1.0)\n",
    "clf_l1.fit(train_x, train_y)\n",
    "\n",
    "print(clf_l1.score(train_x, train_y))\n",
    "print(clf_l1.score(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Democrat Top 10:\n",
      "1 - whereof - 23.8\n",
      "2 - problem - 17.81\n",
      "3 - insofar - 14.23\n",
      "4 - compare - 12.47\n",
      "5 - furthermore - 12.44\n",
      "6 - remarked - 9.38\n",
      "7 - eg - 8.83\n",
      "8 - stern - 8.74\n",
      "9 - peters - 7.89\n",
      "10 - exhibited - 7.44\n",
      "\n",
      "Republican Top 10:\n",
      "1 - isso - -10.07\n",
      "2 - contrast - -9.41\n",
      "3 - sect - -9.29\n",
      "4 - pursuance - -9.08\n",
      "5 - observed - -8.92\n",
      "6 - quote - -7.67\n",
      "7 - besides - -7.56\n",
      "8 - wall - -6.53\n",
      "9 - observing - -6.29\n",
      "10 - argue - -6.13\n"
     ]
    }
   ],
   "source": [
    "# Find top 10 words for both Democrats and Republicans\n",
    "coeff = clf_l1.coef_.flatten()\n",
    "democrat_top_10 = list(sorted(range(len(coeff)), key=lambda i: coeff[i], reverse=True)[0:10])\n",
    "republican_top_10 = list(sorted(range(len(coeff)), key=lambda i: coeff[i])[0:10])\n",
    "\n",
    "print(\"Democrat Top 10:\")\n",
    "for i, d in enumerate(democrat_top_10):\n",
    "    print(i+1, '-', vocab.id_to_word.get(d), '-', round(coeff[d],2))\n",
    "\n",
    "print()\n",
    "print(\"Republican Top 10:\")\n",
    "for i, r in enumerate(republican_top_10):\n",
    "    print(i+1, '-', vocab.id_to_word.get(r), '-', round(coeff[r],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cross_entropy_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(1791.774, 1814.6]</th>\n",
       "      <td>1.329298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1814.6, 1837.2]</th>\n",
       "      <td>0.461046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1837.2, 1859.8]</th>\n",
       "      <td>0.539928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1859.8, 1882.4]</th>\n",
       "      <td>1.642566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1882.4, 1905.0]</th>\n",
       "      <td>1.492848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1905.0, 1927.6]</th>\n",
       "      <td>1.373620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1927.6, 1950.2]</th>\n",
       "      <td>0.921830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1950.2, 1972.8]</th>\n",
       "      <td>0.790945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1972.8, 1995.4]</th>\n",
       "      <td>1.223812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1995.4, 2018.0]</th>\n",
       "      <td>1.248863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    cross_entropy_loss\n",
       "(1791.774, 1814.6]            1.329298\n",
       "(1814.6, 1837.2]              0.461046\n",
       "(1837.2, 1859.8]              0.539928\n",
       "(1859.8, 1882.4]              1.642566\n",
       "(1882.4, 1905.0]              1.492848\n",
       "(1905.0, 1927.6]              1.373620\n",
       "(1927.6, 1950.2]              0.921830\n",
       "(1950.2, 1972.8]              0.790945\n",
       "(1972.8, 1995.4]              1.223812\n",
       "(1995.4, 2018.0]              1.248863"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, _ = pd.cut([int(x) for x in train_data.year_filed], 10, retbins=True)\n",
    "bins = pd.cut([int(x) for x in train_data.year_filed], 10, labels=False)\n",
    "\n",
    "preds = np.log(clf_l1.predict_proba(train_x)[:,1])\n",
    "preds_df = pd.DataFrame({'bins': bins, 'log_prob': preds})\n",
    "\n",
    "agg_df = preds_df.groupby('bins').agg(lambda x: -np.mean(x.log_prob))\n",
    "agg_df.index = out.categories\n",
    "agg_df.columns = ['cross_entropy_loss']\n",
    "\n",
    "agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While removing judge names reduced accuracy, it better identifies the types of language we are intending to capture. Thus, we will keep this modification going forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified Regression Parameters - post 1935 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5912, 10000)\n",
      "(5912,)\n",
      "(739, 10000)\n",
      "(739,)\n",
      "(740, 10000)\n",
      "(740,)\n"
     ]
    }
   ],
   "source": [
    "# Define TF-IDF vectorizer; Using top 1000 words as stop words.\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "# Remove judge names b/c these are strong indicators of political party without indicating how language is used in opinions\n",
    "stop_word_set = [w for w, i in vocab.unigram_counts.most_common(1000)]\n",
    "stop_word_set.extend([judge_name for judge_name in set(w.lower() for w in data.name_last)])\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None,\n",
    "    vocabulary=vocab.word_to_id,\n",
    "    stop_words=stop_word_set)  \n",
    "\n",
    "# Transform to TFIDF data sets\n",
    "\n",
    "train_x = tfidf.fit_transform(train_data_post1935.text)\n",
    "train_y = train_data_post1935.y\n",
    "\n",
    "dev_x = tfidf.transform(dev_data_post1935.text)\n",
    "dev_y = dev_data_post1935.y\n",
    "\n",
    "test_x = tfidf.transform(test_data_post1935.text)\n",
    "test_y = test_data_post1935.y\n",
    "\n",
    "# print shapes for confirmation\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(dev_x.shape)\n",
    "print(dev_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 stats:\n",
      "0.730886332882\n",
      "0.711772665765\n",
      "L2 stats:\n",
      "0.814952638701\n",
      "0.726657645467\n"
     ]
    }
   ],
   "source": [
    "# Fit logitic regression (L1)\n",
    "clf_l1 = linear_model.LogisticRegression(penalty='l1')\n",
    "clf_l1.fit(train_x, train_y)\n",
    "\n",
    "print(\"L1 stats:\")\n",
    "print(clf_l1.score(train_x, train_y))\n",
    "print(clf_l1.score(dev_x, dev_y))\n",
    "\n",
    "# Fit logitic regression (L2)\n",
    "clf_l2 = linear_model.LogisticRegression(penalty='l2')\n",
    "clf_l2.fit(train_x, train_y)\n",
    "\n",
    "print(\"L2 stats:\")\n",
    "print(clf_l2.score(train_x, train_y))\n",
    "print(clf_l2.score(dev_x, dev_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.725473612991\n",
      "0.723951285521\n",
      "\n",
      "0.2\n",
      "0.742895805142\n",
      "0.729364005413\n",
      "\n",
      "0.3\n",
      "0.756596752368\n",
      "0.725304465494\n",
      "\n",
      "0.4\n",
      "0.768437077131\n",
      "0.722598105548\n",
      "\n",
      "0.5\n",
      "0.777909336942\n",
      "0.72801082544\n",
      "\n",
      "0.6\n",
      "0.787550744249\n",
      "0.72801082544\n",
      "\n",
      "0.7\n",
      "0.79702300406\n",
      "0.725304465494\n",
      "\n",
      "0.8\n",
      "0.802943166441\n",
      "0.723951285521\n",
      "\n",
      "0.9\n",
      "0.809032476319\n",
      "0.726657645467\n",
      "\n",
      "1.0\n",
      "0.814952638701\n",
      "0.726657645467\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try different hyperparameters\n",
    "for c in np.arange(0.1, 1.1, 0.1):\n",
    "    clf_l2 = linear_model.LogisticRegression(penalty='l2', C=c)\n",
    "    clf_l2.fit(train_x, train_y)\n",
    "\n",
    "    print(c)\n",
    "    print(clf_l2.score(train_x, train_y))\n",
    "    print(clf_l2.score(dev_x, dev_y))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.52300405954\n",
      "0.499323410014\n",
      "\n",
      "0.2\n",
      "0.527571041949\n",
      "0.499323410014\n",
      "\n",
      "0.3\n",
      "0.64851150203\n",
      "0.661705006766\n",
      "\n",
      "0.4\n",
      "0.674391069012\n",
      "0.695534506089\n",
      "\n",
      "0.5\n",
      "0.688261163735\n",
      "0.709066305819\n",
      "\n",
      "0.6\n",
      "0.697564276049\n",
      "0.703653585927\n",
      "\n",
      "0.7\n",
      "0.707882273342\n",
      "0.707713125846\n",
      "\n",
      "0.8\n",
      "0.716170500677\n",
      "0.719891745602\n",
      "\n",
      "0.9\n",
      "0.723443843031\n",
      "0.718538565629\n",
      "\n",
      "1.0\n",
      "0.730886332882\n",
      "0.711772665765\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try different hyperparameters\n",
    "for c in np.arange(0.1, 1.1, 0.1):\n",
    "    clf_l1 = linear_model.LogisticRegression(penalty='l1', C=c)\n",
    "    clf_l1.fit(train_x, train_y)\n",
    "\n",
    "    print(c)\n",
    "    print(clf_l1.score(train_x, train_y))\n",
    "    print(clf_l1.score(dev_x, dev_y))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.777909336942\n",
      "0.675675675676\n"
     ]
    }
   ],
   "source": [
    "# Final model with L1 regularization with C = 1.0\n",
    "clf_l2 = linear_model.LogisticRegression(penalty='l2', C=0.5)\n",
    "clf_l2.fit(train_x, train_y)\n",
    "\n",
    "print(clf_l2.score(train_x, train_y))\n",
    "print(clf_l2.score(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Democrat Top 10:\n",
      "1 - problem - 1.12\n",
      "2 - eg - 0.98\n",
      "3 - contempt - 0.87\n",
      "4 - usc - 0.86\n",
      "5 - army - 0.85\n",
      "6 - pointed - 0.82\n",
      "7 - railroads - 0.81\n",
      "8 - enemy - 0.79\n",
      "9 - administrator - 0.78\n",
      "10 - cite - 0.78\n",
      "\n",
      "Republican Top 10:\n",
      "1 - governmental - -1.06\n",
      "2 - subsection - -0.98\n",
      "3 - collateral - -0.91\n",
      "4 - quoting - -0.89\n",
      "5 - briefs - -0.88\n",
      "6 - text - -0.86\n",
      "7 - argue - -0.86\n",
      "8 - approach - -0.86\n",
      "9 - prosecutor - -0.85\n",
      "10 - massachusetts - -0.84\n"
     ]
    }
   ],
   "source": [
    "# Find top 10 words for both Democrats and Republicans\n",
    "coeff = clf_l2.coef_.flatten()\n",
    "democrat_top_10 = list(sorted(range(len(coeff)), key=lambda i: coeff[i], reverse=True)[0:10])\n",
    "republican_top_10 = list(sorted(range(len(coeff)), key=lambda i: coeff[i])[0:10])\n",
    "\n",
    "print(\"Democrat Top 10:\")\n",
    "for i, d in enumerate(democrat_top_10):\n",
    "    print(i+1, '-', vocab.id_to_word.get(d), '-', round(coeff[d],2))\n",
    "\n",
    "print()\n",
    "print(\"Republican Top 10:\")\n",
    "for i, r in enumerate(republican_top_10):\n",
    "    print(i+1, '-', vocab.id_to_word.get(r), '-', round(coeff[r],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cross_entropy_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(1935.918, 1944.2]</th>\n",
       "      <td>0.516035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1944.2, 1952.4]</th>\n",
       "      <td>0.451311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1952.4, 1960.6]</th>\n",
       "      <td>0.553444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1960.6, 1968.8]</th>\n",
       "      <td>0.674378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1968.8, 1977.0]</th>\n",
       "      <td>0.905125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1977.0, 1985.2]</th>\n",
       "      <td>1.031530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1985.2, 1993.4]</th>\n",
       "      <td>1.098933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1993.4, 2001.6]</th>\n",
       "      <td>1.140037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2001.6, 2009.8]</th>\n",
       "      <td>1.099959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2009.8, 2018.0]</th>\n",
       "      <td>0.755164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    cross_entropy_loss\n",
       "(1935.918, 1944.2]            0.516035\n",
       "(1944.2, 1952.4]              0.451311\n",
       "(1952.4, 1960.6]              0.553444\n",
       "(1960.6, 1968.8]              0.674378\n",
       "(1968.8, 1977.0]              0.905125\n",
       "(1977.0, 1985.2]              1.031530\n",
       "(1985.2, 1993.4]              1.098933\n",
       "(1993.4, 2001.6]              1.140037\n",
       "(2001.6, 2009.8]              1.099959\n",
       "(2009.8, 2018.0]              0.755164"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, _ = pd.cut([int(x) for x in train_data_post1935.year_filed], 10, retbins=True)\n",
    "bins = pd.cut([int(x) for x in train_data_post1935.year_filed], 10, labels=False)\n",
    "\n",
    "preds = np.log(clf_l2.predict_proba(train_x)[:,1])\n",
    "preds_df = pd.DataFrame({'bins': bins, 'log_prob': preds})\n",
    "\n",
    "agg_df = preds_df.groupby('bins').agg(lambda x: -np.mean(x.log_prob))\n",
    "agg_df.index = out.categories\n",
    "agg_df.columns = ['cross_entropy_loss']\n",
    "\n",
    "agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: reducing the time period did not increase accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified Regression Parameters - balanced class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17052, 10000)\n",
      "(17052,)\n",
      "(2131, 10000)\n",
      "(2131,)\n",
      "(2133, 10000)\n",
      "(2133,)\n"
     ]
    }
   ],
   "source": [
    "# Define TF-IDF vectorizer; Using top 1000 words as stop words.\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "# Remove judge names b/c these are strong indicators of political party without indicating how language is used in opinions\n",
    "stop_word_set = [w for w, i in vocab.unigram_counts.most_common(1000)]\n",
    "stop_word_set.extend([judge_name for judge_name in set(w.lower() for w in data.name_last)])\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None,\n",
    "    vocabulary=vocab.word_to_id,\n",
    "    stop_words=stop_word_set)  \n",
    "\n",
    "# Transform to TFIDF data sets\n",
    "\n",
    "train_x = tfidf.fit_transform(train_data.text)\n",
    "train_y = train_data.y\n",
    "\n",
    "dev_x = tfidf.transform(dev_data.text)\n",
    "dev_y = dev_data.y\n",
    "\n",
    "test_x = tfidf.transform(test_data.text)\n",
    "test_y = test_data.y\n",
    "\n",
    "# print shapes for confirmation\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(dev_x.shape)\n",
    "print(dev_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 stats:\n",
      "0.782782078349\n",
      "0.719380572501\n",
      "L2 stats:\n",
      "0.819200093831\n",
      "0.717034256218\n"
     ]
    }
   ],
   "source": [
    "# Fit logitic regression (L1)\n",
    "clf_l1 = linear_model.LogisticRegression(penalty='l1', class_weight='balanced')\n",
    "clf_l1.fit(train_x, train_y)\n",
    "\n",
    "print(\"L1 stats:\")\n",
    "print(clf_l1.score(train_x, train_y))\n",
    "print(clf_l1.score(dev_x, dev_y))\n",
    "\n",
    "# Fit logitic regression (L2)\n",
    "clf_l2 = linear_model.LogisticRegression(penalty='l2', class_weight='balanced')\n",
    "clf_l2.fit(train_x, train_y)\n",
    "\n",
    "print(\"L2 stats:\")\n",
    "print(clf_l2.score(train_x, train_y))\n",
    "print(clf_l2.score(dev_x, dev_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.641391039174\n",
      "0.654152979822\n",
      "\n",
      "0.2\n",
      "0.665317851278\n",
      "0.672923510089\n",
      "\n",
      "0.3\n",
      "0.684729064039\n",
      "0.676208352886\n",
      "\n",
      "0.4\n",
      "0.701618578466\n",
      "0.682308775223\n",
      "\n",
      "0.5\n",
      "0.721440300258\n",
      "0.690286250587\n",
      "\n",
      "0.6\n",
      "0.737508796622\n",
      "0.704364148287\n",
      "\n",
      "0.7\n",
      "0.752580342482\n",
      "0.717034256218\n",
      "\n",
      "0.8\n",
      "0.762373915083\n",
      "0.720319099015\n",
      "\n",
      "0.9\n",
      "0.773516303073\n",
      "0.718911309244\n",
      "\n",
      "1.0\n",
      "0.782840722496\n",
      "0.719380572501\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try different hyperparameters\n",
    "for c in np.arange(0.1, 1.1, 0.1):\n",
    "    clf_l1 = linear_model.LogisticRegression(penalty='l1', C=c, class_weight='balanced')\n",
    "    clf_l1.fit(train_x, train_y)\n",
    "\n",
    "    print(c)\n",
    "    print(clf_l1.score(train_x, train_y))\n",
    "    print(clf_l1.score(dev_x, dev_y))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.782840722496\n",
      "0.704172526957\n"
     ]
    }
   ],
   "source": [
    "# Final model with L1 regularization with C = 1.0\n",
    "clf_l1 = linear_model.LogisticRegression(penalty='l1', C=1.0, class_weight='balanced')\n",
    "clf_l1.fit(train_x, train_y)\n",
    "\n",
    "print(clf_l1.score(train_x, train_y))\n",
    "print(clf_l1.score(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Democrat Top 10:\n",
      "1 - whereof - 24.25\n",
      "2 - problem - 18.45\n",
      "3 - insofar - 15.1\n",
      "4 - furthermore - 13.93\n",
      "5 - compare - 12.92\n",
      "6 - remarked - 9.84\n",
      "7 - eg - 9.22\n",
      "8 - stern - 8.59\n",
      "9 - whilst - 8.17\n",
      "10 - peters - 8.14\n",
      "\n",
      "Republican Top 10:\n",
      "1 - isso - -12.17\n",
      "2 - contrast - -10.37\n",
      "3 - sect - -10.14\n",
      "4 - quote - -8.98\n",
      "5 - observing - -8.93\n",
      "6 - pursuance - -8.93\n",
      "7 - observed - -8.83\n",
      "8 - besides - -8.59\n",
      "9 - argue - -6.38\n",
      "10 - wall - -6.18\n"
     ]
    }
   ],
   "source": [
    "# Find top 10 words for both Democrats and Republicans\n",
    "coeff = clf_l1.coef_.flatten()\n",
    "democrat_top_10 = list(sorted(range(len(coeff)), key=lambda i: coeff[i], reverse=True)[0:10])\n",
    "republican_top_10 = list(sorted(range(len(coeff)), key=lambda i: coeff[i])[0:10])\n",
    "\n",
    "print(\"Democrat Top 10:\")\n",
    "for i, d in enumerate(democrat_top_10):\n",
    "    print(i+1, '-', vocab.id_to_word.get(d), '-', round(coeff[d],2))\n",
    "\n",
    "print()\n",
    "print(\"Republican Top 10:\")\n",
    "for i, r in enumerate(republican_top_10):\n",
    "    print(i+1, '-', vocab.id_to_word.get(r), '-', round(coeff[r],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cross_entropy_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(1791.774, 1803.3]</th>\n",
       "      <td>0.930352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1803.3, 1814.6]</th>\n",
       "      <td>0.189055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1814.6, 1825.9]</th>\n",
       "      <td>0.265687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1825.9, 1837.2]</th>\n",
       "      <td>0.288959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1837.2, 1848.5]</th>\n",
       "      <td>0.460992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1848.5, 1859.8]</th>\n",
       "      <td>1.014621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1859.8, 1871.1]</th>\n",
       "      <td>1.466419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1871.1, 1882.4]</th>\n",
       "      <td>1.179084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1882.4, 1893.7]</th>\n",
       "      <td>0.989033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1893.7, 1905.0]</th>\n",
       "      <td>1.095646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1905.0, 1916.3]</th>\n",
       "      <td>0.948085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1916.3, 1927.6]</th>\n",
       "      <td>0.841003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1927.6, 1938.9]</th>\n",
       "      <td>0.543174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1938.9, 1950.2]</th>\n",
       "      <td>0.406495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1950.2, 1961.5]</th>\n",
       "      <td>0.503400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1961.5, 1972.8]</th>\n",
       "      <td>0.647846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1972.8, 1984.1]</th>\n",
       "      <td>0.833972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1984.1, 1995.4]</th>\n",
       "      <td>0.969375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1995.4, 2006.7]</th>\n",
       "      <td>0.935680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2006.7, 2018.0]</th>\n",
       "      <td>0.629872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    cross_entropy_loss\n",
       "(1791.774, 1803.3]            0.930352\n",
       "(1803.3, 1814.6]              0.189055\n",
       "(1814.6, 1825.9]              0.265687\n",
       "(1825.9, 1837.2]              0.288959\n",
       "(1837.2, 1848.5]              0.460992\n",
       "(1848.5, 1859.8]              1.014621\n",
       "(1859.8, 1871.1]              1.466419\n",
       "(1871.1, 1882.4]              1.179084\n",
       "(1882.4, 1893.7]              0.989033\n",
       "(1893.7, 1905.0]              1.095646\n",
       "(1905.0, 1916.3]              0.948085\n",
       "(1916.3, 1927.6]              0.841003\n",
       "(1927.6, 1938.9]              0.543174\n",
       "(1938.9, 1950.2]              0.406495\n",
       "(1950.2, 1961.5]              0.503400\n",
       "(1961.5, 1972.8]              0.647846\n",
       "(1972.8, 1984.1]              0.833972\n",
       "(1984.1, 1995.4]              0.969375\n",
       "(1995.4, 2006.7]              0.935680\n",
       "(2006.7, 2018.0]              0.629872"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, _ = pd.cut([int(x) for x in train_data.year_filed], 20, retbins=True)\n",
    "bins = pd.cut([int(x) for x in train_data.year_filed], 22, labels=False)\n",
    "\n",
    "preds = np.log(clf_l1.predict_proba(train_x)[:,1])\n",
    "preds_df = pd.DataFrame({'bins': bins, 'log_prob': preds})\n",
    "\n",
    "agg_df = preds_df.groupby('bins').agg(lambda x: -np.mean(x.log_prob))\n",
    "agg_df.index = out.categories\n",
    "agg_df.columns = ['cross_entropy_loss']\n",
    "\n",
    "agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: weighted class distribution did not improve performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified Regression Parameters - larger vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17052, 12314)\n",
      "(17052,)\n",
      "(2131, 12314)\n",
      "(2131,)\n",
      "(2133, 12314)\n",
      "(2133,)\n"
     ]
    }
   ],
   "source": [
    "# Define TF-IDF vectorizer; Using top 1000 words as stop words.\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "# Remove judge names b/c these are strong indicators of political party without indicating how language is used in opinions\n",
    "stop_word_set = [w for w, i in vocab_12314.unigram_counts.most_common(1000)]\n",
    "stop_word_set.extend([judge_name for judge_name in set(w.lower() for w in data.name_last)])\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None,\n",
    "    vocabulary=vocab_12314.word_to_id,\n",
    "    stop_words=stop_word_set)  \n",
    "\n",
    "# Transform to TFIDF data sets\n",
    "\n",
    "train_x = tfidf.fit_transform(train_data.text)\n",
    "train_y = train_data.y\n",
    "\n",
    "dev_x = tfidf.transform(dev_data.text)\n",
    "dev_y = dev_data.y\n",
    "\n",
    "test_x = tfidf.transform(test_data.text)\n",
    "test_y = test_data.y\n",
    "\n",
    "# print shapes for confirmation\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(dev_x.shape)\n",
    "print(dev_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 stats:\n",
      "0.771287825475\n",
      "0.736274049742\n",
      "L2 stats:\n",
      "0.803952615529\n",
      "0.723603941811\n"
     ]
    }
   ],
   "source": [
    "# Fit logitic regression (L1)\n",
    "clf_l1 = linear_model.LogisticRegression(penalty='l1')\n",
    "clf_l1.fit(train_x, train_y)\n",
    "\n",
    "print(\"L1 stats:\")\n",
    "print(clf_l1.score(train_x, train_y))\n",
    "print(clf_l1.score(dev_x, dev_y))\n",
    "\n",
    "# Fit logitic regression (L2)\n",
    "clf_l2 = linear_model.LogisticRegression(penalty='l2')\n",
    "clf_l2.fit(train_x, train_y)\n",
    "\n",
    "print(\"L2 stats:\")\n",
    "print(clf_l2.score(train_x, train_y))\n",
    "print(clf_l2.score(dev_x, dev_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.661447337556\n",
      "0.660722665415\n",
      "\n",
      "0.2\n",
      "0.675639221206\n",
      "0.678554669169\n",
      "\n",
      "0.3\n",
      "0.68930330753\n",
      "0.68840919756\n",
      "\n",
      "0.4\n",
      "0.702498240676\n",
      "0.697325199437\n",
      "\n",
      "0.5\n",
      "0.715869106263\n",
      "0.702956358517\n",
      "\n",
      "0.6\n",
      "0.727656579873\n",
      "0.713280150164\n",
      "\n",
      "0.7\n",
      "0.740089139104\n",
      "0.718442045988\n",
      "\n",
      "0.8\n",
      "0.752638986629\n",
      "0.721726888785\n",
      "\n",
      "0.9\n",
      "0.761025099695\n",
      "0.727827311122\n",
      "\n",
      "1.0\n",
      "0.771287825475\n",
      "0.736274049742\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try different hyperparameters\n",
    "for c in np.arange(0.1, 1.1, 0.1):\n",
    "    clf_l1 = linear_model.LogisticRegression(penalty='l1', C=c)\n",
    "    clf_l1.fit(train_x, train_y)\n",
    "\n",
    "    print(c)\n",
    "    print(clf_l1.score(train_x, train_y))\n",
    "    print(clf_l1.score(dev_x, dev_y))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.771287825475\n",
      "0.719174871074\n"
     ]
    }
   ],
   "source": [
    "# Final model with L1 regularization with C = 1.0\n",
    "clf_l1 = linear_model.LogisticRegression(penalty='l1', C=1.0)\n",
    "clf_l1.fit(train_x, train_y)\n",
    "\n",
    "print(clf_l1.score(train_x, train_y))\n",
    "print(clf_l1.score(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Democrat Top 10:\n",
      "1 - whereof - 24.11\n",
      "2 - problem - 18.47\n",
      "3 - insofar - 14.78\n",
      "4 - compare - 12.74\n",
      "5 - furthermore - 12.34\n",
      "6 - remarked - 9.39\n",
      "7 - eg - 8.92\n",
      "8 - stern - 8.64\n",
      "9 - peters - 8.0\n",
      "10 - exhibited - 7.48\n",
      "\n",
      "Republican Top 10:\n",
      "1 - isso - -10.06\n",
      "2 - sect - -9.56\n",
      "3 - observed - -9.32\n",
      "4 - pursuance - -9.29\n",
      "5 - contrast - -9.11\n",
      "6 - quote - -7.65\n",
      "7 - besides - -7.48\n",
      "8 - wall - -6.63\n",
      "9 - observing - -5.88\n",
      "10 - anything - -5.77\n"
     ]
    }
   ],
   "source": [
    "# Find top 10 words for both Democrats and Republicans\n",
    "coeff = clf_l1.coef_.flatten()\n",
    "democrat_top_10 = list(sorted(range(len(coeff)), key=lambda i: coeff[i], reverse=True)[0:10])\n",
    "republican_top_10 = list(sorted(range(len(coeff)), key=lambda i: coeff[i])[0:10])\n",
    "\n",
    "print(\"Democrat Top 10:\")\n",
    "for i, d in enumerate(democrat_top_10):\n",
    "    print(i+1, '-', vocab_12314.id_to_word.get(d), '-', round(coeff[d],2))\n",
    "\n",
    "print()\n",
    "print(\"Republican Top 10:\")\n",
    "for i, r in enumerate(republican_top_10):\n",
    "    print(i+1, '-', vocab_12314.id_to_word.get(r), '-', round(coeff[r],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cross_entropy_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(1791.774, 1814.6]</th>\n",
       "      <td>1.339322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1814.6, 1837.2]</th>\n",
       "      <td>0.462582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1837.2, 1859.8]</th>\n",
       "      <td>0.547575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1859.8, 1882.4]</th>\n",
       "      <td>1.632333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1882.4, 1905.0]</th>\n",
       "      <td>1.483389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1905.0, 1927.6]</th>\n",
       "      <td>1.366678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1927.6, 1950.2]</th>\n",
       "      <td>0.922225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1950.2, 1972.8]</th>\n",
       "      <td>0.793996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1972.8, 1995.4]</th>\n",
       "      <td>1.215459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1995.4, 2018.0]</th>\n",
       "      <td>1.240703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    cross_entropy_loss\n",
       "(1791.774, 1814.6]            1.339322\n",
       "(1814.6, 1837.2]              0.462582\n",
       "(1837.2, 1859.8]              0.547575\n",
       "(1859.8, 1882.4]              1.632333\n",
       "(1882.4, 1905.0]              1.483389\n",
       "(1905.0, 1927.6]              1.366678\n",
       "(1927.6, 1950.2]              0.922225\n",
       "(1950.2, 1972.8]              0.793996\n",
       "(1972.8, 1995.4]              1.215459\n",
       "(1995.4, 2018.0]              1.240703"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, _ = pd.cut([int(x) for x in train_data.year_filed], 10, retbins=True)\n",
    "bins = pd.cut([int(x) for x in train_data.year_filed], 10, labels=False)\n",
    "\n",
    "preds = np.log(clf_l1.predict_proba(train_x)[:,1])\n",
    "preds_df = pd.DataFrame({'bins': bins, 'log_prob': preds})\n",
    "\n",
    "agg_df = preds_df.groupby('bins').agg(lambda x: -np.mean(x.log_prob))\n",
    "agg_df.index = out.categories\n",
    "agg_df.columns = ['cross_entropy_loss']\n",
    "\n",
    "agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: practically the same results as vocab of 10,000 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified Regression Parameters - smaller vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17052, 6155)\n",
      "(17052,)\n",
      "(2131, 6155)\n",
      "(2131,)\n",
      "(2133, 6155)\n",
      "(2133,)\n"
     ]
    }
   ],
   "source": [
    "# Define TF-IDF vectorizer; Using top 1000 words as stop words.\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "# Remove judge names b/c these are strong indicators of political party without indicating how language is used in opinions\n",
    "stop_word_set = [w for w, i in vocab_6155.unigram_counts.most_common(1000)]\n",
    "stop_word_set.extend([judge_name for judge_name in set(w.lower() for w in data.name_last)])\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None,\n",
    "    vocabulary=vocab_6155.word_to_id,\n",
    "    stop_words=stop_word_set)  \n",
    "\n",
    "# Transform to TFIDF data sets\n",
    "\n",
    "train_x = tfidf.fit_transform(train_data.text)\n",
    "train_y = train_data.y\n",
    "\n",
    "dev_x = tfidf.transform(dev_data.text)\n",
    "dev_y = dev_data.y\n",
    "\n",
    "test_x = tfidf.transform(test_data.text)\n",
    "test_y = test_data.y\n",
    "\n",
    "# print shapes for confirmation\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(dev_x.shape)\n",
    "print(dev_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 stats:\n",
      "0.776331222144\n",
      "0.729704364148\n",
      "L2 stats:\n",
      "0.790464461647\n",
      "0.724542468325\n"
     ]
    }
   ],
   "source": [
    "# Fit logitic regression (L1)\n",
    "clf_l1 = linear_model.LogisticRegression(penalty='l1')\n",
    "clf_l1.fit(train_x, train_y)\n",
    "\n",
    "print(\"L1 stats:\")\n",
    "print(clf_l1.score(train_x, train_y))\n",
    "print(clf_l1.score(dev_x, dev_y))\n",
    "\n",
    "# Fit logitic regression (L2)\n",
    "clf_l2 = linear_model.LogisticRegression(penalty='l2')\n",
    "clf_l2.fit(train_x, train_y)\n",
    "\n",
    "print(\"L2 stats:\")\n",
    "print(clf_l2.score(train_x, train_y))\n",
    "print(clf_l2.score(dev_x, dev_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.661740558292\n",
      "0.660722665415\n",
      "\n",
      "0.2\n",
      "0.67992024396\n",
      "0.683247301736\n",
      "\n",
      "0.3\n",
      "0.693936195168\n",
      "0.695917409667\n",
      "\n",
      "0.4\n",
      "0.709125029322\n",
      "0.701548568747\n",
      "\n",
      "0.5\n",
      "0.723610133709\n",
      "0.706241201314\n",
      "\n",
      "0.6\n",
      "0.736805066854\n",
      "0.714218676678\n",
      "\n",
      "0.7\n",
      "0.748827117054\n",
      "0.717034256218\n",
      "\n",
      "0.8\n",
      "0.760731878958\n",
      "0.719849835758\n",
      "\n",
      "0.9\n",
      "0.769528501056\n",
      "0.726419521351\n",
      "\n",
      "1.0\n",
      "0.776331222144\n",
      "0.729704364148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try different hyperparameters\n",
    "for c in np.arange(0.1, 1.1, 0.1):\n",
    "    clf_l1 = linear_model.LogisticRegression(penalty='l1', C=c)\n",
    "    clf_l1.fit(train_x, train_y)\n",
    "\n",
    "    print(c)\n",
    "    print(clf_l1.score(train_x, train_y))\n",
    "    print(clf_l1.score(dev_x, dev_y))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.776331222144\n",
      "0.721050164088\n"
     ]
    }
   ],
   "source": [
    "# Final model with L1 regularization with C = 1.0\n",
    "clf_l1 = linear_model.LogisticRegression(penalty='l1', C=1.0)\n",
    "clf_l1.fit(train_x, train_y)\n",
    "\n",
    "print(clf_l1.score(train_x, train_y))\n",
    "print(clf_l1.score(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Democrat Top 10:\n",
      "1 - whereof - 19.33\n",
      "2 - problem - 16.5\n",
      "3 - insofar - 13.56\n",
      "4 - furthermore - 12.68\n",
      "5 - compare - 11.8\n",
      "6 - remarked - 8.87\n",
      "7 - eg - 8.87\n",
      "8 - peters - 7.5\n",
      "9 - likewise - 7.39\n",
      "10 - exhibited - 7.29\n",
      "\n",
      "Republican Top 10:\n",
      "1 - contrast - -10.31\n",
      "2 - sect - -8.75\n",
      "3 - pursuance - -8.56\n",
      "4 - observed - -8.39\n",
      "5 - quote - -7.63\n",
      "6 - besides - -7.21\n",
      "7 - argue - -6.8\n",
      "8 - observing - -6.62\n",
      "9 - wall - -5.9\n",
      "10 - whereas - -5.66\n"
     ]
    }
   ],
   "source": [
    "# Find top 10 words for both Democrats and Republicans\n",
    "coeff = clf_l1.coef_.flatten()\n",
    "democrat_top_10 = list(sorted(range(len(coeff)), key=lambda i: coeff[i], reverse=True)[0:10])\n",
    "republican_top_10 = list(sorted(range(len(coeff)), key=lambda i: coeff[i])[0:10])\n",
    "\n",
    "print(\"Democrat Top 10:\")\n",
    "for i, d in enumerate(democrat_top_10):\n",
    "    print(i+1, '-', vocab_6155.id_to_word.get(d), '-', round(coeff[d],2))\n",
    "\n",
    "print()\n",
    "print(\"Republican Top 10:\")\n",
    "for i, r in enumerate(republican_top_10):\n",
    "    print(i+1, '-', vocab_6155.id_to_word.get(r), '-', round(coeff[r],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cross_entropy_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(1791.774, 1814.6]</th>\n",
       "      <td>1.306647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1814.6, 1837.2]</th>\n",
       "      <td>0.406512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1837.2, 1859.8]</th>\n",
       "      <td>0.536763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1859.8, 1882.4]</th>\n",
       "      <td>1.634456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1882.4, 1905.0]</th>\n",
       "      <td>1.522472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1905.0, 1927.6]</th>\n",
       "      <td>1.397685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1927.6, 1950.2]</th>\n",
       "      <td>0.919636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1950.2, 1972.8]</th>\n",
       "      <td>0.783053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1972.8, 1995.4]</th>\n",
       "      <td>1.253913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1995.4, 2018.0]</th>\n",
       "      <td>1.273757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    cross_entropy_loss\n",
       "(1791.774, 1814.6]            1.306647\n",
       "(1814.6, 1837.2]              0.406512\n",
       "(1837.2, 1859.8]              0.536763\n",
       "(1859.8, 1882.4]              1.634456\n",
       "(1882.4, 1905.0]              1.522472\n",
       "(1905.0, 1927.6]              1.397685\n",
       "(1927.6, 1950.2]              0.919636\n",
       "(1950.2, 1972.8]              0.783053\n",
       "(1972.8, 1995.4]              1.253913\n",
       "(1995.4, 2018.0]              1.273757"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, _ = pd.cut([int(x) for x in train_data.year_filed], 10, retbins=True)\n",
    "bins = pd.cut([int(x) for x in train_data.year_filed], 10, labels=False)\n",
    "\n",
    "preds = np.log(clf_l1.predict_proba(train_x)[:,1])\n",
    "preds_df = pd.DataFrame({'bins': bins, 'log_prob': preds})\n",
    "\n",
    "agg_df = preds_df.groupby('bins').agg(lambda x: -np.mean(x.log_prob))\n",
    "agg_df.index = out.categories\n",
    "agg_df.columns = ['cross_entropy_loss']\n",
    "\n",
    "agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: practically the same as the larger vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified Regression Parameters - smaller set of stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: these are the values used in presentation slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17052, 10000)\n",
      "(17052,)\n",
      "(2131, 10000)\n",
      "(2131,)\n",
      "(2133, 10000)\n",
      "(2133,)\n"
     ]
    }
   ],
   "source": [
    "# Define TF-IDF vectorizer; Using top 1000 words as stop words.\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "# Remove judge names b/c these are strong indicators of political party without indicating how language is used in opinions\n",
    "# Stop word set up to position 200, corresponds to 99.65 percentile with words having counts> 38746\n",
    "# Still includes common words like 'petitioners', but IDF weighting should take care of that \n",
    "stop_word_set = [w for w, i in vocab.unigram_counts.most_common(200)]\n",
    "stop_word_set.extend([judge_name for judge_name in set(w.lower() for w in data.name_last)])\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None,\n",
    "    vocabulary=vocab.word_to_id,\n",
    "    stop_words=stop_word_set)  \n",
    "\n",
    "# Transform to TFIDF data sets\n",
    "\n",
    "train_x = tfidf.fit_transform(train_data.text)\n",
    "train_y = train_data.y\n",
    "\n",
    "dev_x = tfidf.transform(dev_data.text)\n",
    "dev_y = dev_data.y\n",
    "\n",
    "test_x = tfidf.transform(test_data.text)\n",
    "test_y = test_data.y\n",
    "\n",
    "# print shapes for confirmation\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(dev_x.shape)\n",
    "print(dev_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 stats:\n",
      "0.789643443584\n",
      "0.763022055373\n",
      "L2 stats:\n",
      "0.809523809524\n",
      "0.756452369779\n"
     ]
    }
   ],
   "source": [
    "# Fit logitic regression (L1)\n",
    "clf_l1 = linear_model.LogisticRegression(penalty='l1')\n",
    "clf_l1.fit(train_x, train_y)\n",
    "\n",
    "print(\"L1 stats:\")\n",
    "print(clf_l1.score(train_x, train_y))\n",
    "print(clf_l1.score(dev_x, dev_y))\n",
    "\n",
    "# Fit logitic regression (L2)\n",
    "clf_l2 = linear_model.LogisticRegression(penalty='l2')\n",
    "clf_l2.fit(train_x, train_y)\n",
    "\n",
    "print(\"L2 stats:\")\n",
    "print(clf_l2.score(train_x, train_y))\n",
    "print(clf_l2.score(dev_x, dev_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.663793103448\n",
      "0.661191928672\n",
      "\n",
      "0.2\n",
      "0.681503635937\n",
      "0.679023932426\n",
      "\n",
      "0.3\n",
      "0.700621627962\n",
      "0.699202252464\n",
      "\n",
      "0.4\n",
      "0.719035890218\n",
      "0.715626466448\n",
      "\n",
      "0.5\n",
      "0.734224724373\n",
      "0.729235100892\n",
      "\n",
      "0.6\n",
      "0.749296270232\n",
      "0.738620366025\n",
      "\n",
      "0.7\n",
      "0.760321369927\n",
      "0.746128578132\n",
      "\n",
      "0.8\n",
      "0.771581046212\n",
      "0.752229000469\n",
      "\n",
      "0.9\n",
      "0.78078817734\n",
      "0.76067573909\n",
      "\n",
      "1.0\n",
      "0.789643443584\n",
      "0.763022055373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try different hyperparameters for L1\n",
    "for c in np.arange(0.1, 1.1, 0.1):\n",
    "    clf_l1 = linear_model.LogisticRegression(penalty='l1', C=c)\n",
    "    clf_l1.fit(train_x, train_y)\n",
    "\n",
    "    print(c)\n",
    "    print(clf_l1.score(train_x, train_y))\n",
    "    print(clf_l1.score(dev_x, dev_y))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.677339901478\n",
      "0.670107930549\n",
      "\n",
      "0.2\n",
      "0.7107084213\n",
      "0.694509619897\n",
      "\n",
      "0.3\n",
      "0.734459300962\n",
      "0.717503519474\n",
      "\n",
      "0.4\n",
      "0.753342716397\n",
      "0.728765837635\n",
      "\n",
      "0.5\n",
      "0.767417311752\n",
      "0.733458470202\n",
      "\n",
      "0.6\n",
      "0.778266479005\n",
      "0.742374472079\n",
      "\n",
      "0.7\n",
      "0.78776683087\n",
      "0.746128578132\n",
      "\n",
      "0.8\n",
      "0.79562514661\n",
      "0.748474894416\n",
      "\n",
      "0.9\n",
      "0.802603800141\n",
      "0.753636790239\n",
      "\n",
      "1.0\n",
      "0.809523809524\n",
      "0.756452369779\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try different hyperparameters for L2\n",
    "for c in np.arange(0.1, 1.1, 0.1):\n",
    "    clf_l2 = linear_model.LogisticRegression(penalty='l2', C=c)\n",
    "    clf_l2.fit(train_x, train_y)\n",
    "\n",
    "    print(c)\n",
    "    print(clf_l2.score(train_x, train_y))\n",
    "    print(clf_l2.score(dev_x, dev_y))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 model results:\n",
      "0.789643443584\n",
      "0.743084857009\n",
      "L2 model results:\n",
      "0.809523809524\n",
      "0.732301922175\n"
     ]
    }
   ],
   "source": [
    "# Final model with L1 regularization with C = 1.0\n",
    "clf_l1 = linear_model.LogisticRegression(penalty='l1', C=1.0)\n",
    "clf_l1.fit(train_x, train_y)\n",
    "\n",
    "print(\"L1 model results:\")\n",
    "print(clf_l1.score(train_x, train_y))\n",
    "print(clf_l1.score(test_x, test_y))\n",
    "\n",
    "# Final model with L1 regularization with C = 1.0\n",
    "clf_l2 = linear_model.LogisticRegression(penalty='l2', C=1.0)\n",
    "clf_l2.fit(train_x, train_y)\n",
    "\n",
    "print(\"L2 model results:\")\n",
    "print(clf_l2.score(train_x, train_y))\n",
    "print(clf_l2.score(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 results:\n",
      "Democrat Top 20:\n",
      "1 - whereof - 21.65\n",
      "2 - argued - 21.5\n",
      "3 - problem - 19.04\n",
      "4 - compare - 13.03\n",
      "5 - remarked - 10.02\n",
      "6 - eg - 9.92\n",
      "7 - ruled - 9.31\n",
      "8 - furthermore - 9.01\n",
      "9 - concerning - 8.62\n",
      "10 - introduced - 8.59\n",
      "11 - peters - 8.46\n",
      "12 - hereby - 8.19\n",
      "13 - insofar - 8.15\n",
      "14 - therein - 7.77\n",
      "15 - prior - 7.69\n",
      "16 - exhibited - 7.53\n",
      "17 - likewise - 7.52\n",
      "18 - moreover - 7.49\n",
      "19 - thereby - 7.0\n",
      "20 - inasmuch - 6.91\n",
      "\n",
      "Republican Top 20:\n",
      "1 - delivered - -10.15\n",
      "2 - argue - -10.14\n",
      "3 - simply - -9.42\n",
      "4 - sect - -8.84\n",
      "5 - pursuance - -8.44\n",
      "6 - far - -8.11\n",
      "7 - shown - -7.75\n",
      "8 - consider - -7.75\n",
      "9 - latter - -7.49\n",
      "10 - follows - -7.1\n",
      "11 - wall - -7.04\n",
      "12 - analysis - -6.86\n",
      "13 - besides - -6.48\n",
      "14 - nothing - -6.45\n",
      "15 - id - -6.2\n",
      "16 - least - -5.73\n",
      "17 - observed - -5.67\n",
      "18 - ruling - -5.61\n",
      "19 - brought - -5.49\n",
      "20 - principle - -5.47\n"
     ]
    }
   ],
   "source": [
    "# Find top 10 words for both Democrats and Republicans for L1 regularization\n",
    "coeff = clf_l1.coef_.flatten()\n",
    "democrat_top_20 = list(sorted(range(len(coeff)), key=lambda i: coeff[i], reverse=True)[0:20])\n",
    "republican_top_20 = list(sorted(range(len(coeff)), key=lambda i: coeff[i])[0:20])\n",
    "\n",
    "print(\"L1 results:\")\n",
    "\n",
    "print(\"Democrat Top 20:\")\n",
    "for i, d in enumerate(democrat_top_20):\n",
    "    print(i+1, '-', vocab.id_to_word.get(d), '-', round(coeff[d],2))\n",
    "\n",
    "print()\n",
    "print(\"Republican Top 20:\")\n",
    "for i, r in enumerate(republican_top_20):\n",
    "    print(i+1, '-', vocab.id_to_word.get(r), '-', round(coeff[r],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 results:\n",
      "Democrat Top 20:\n",
      "1 - argued - 6.15\n",
      "2 - prior - 3.43\n",
      "3 - hereby - 3.41\n",
      "4 - whereof - 3.41\n",
      "5 - peters - 3.22\n",
      "6 - problem - 2.98\n",
      "7 - legal - 2.78\n",
      "8 - therein - 2.72\n",
      "9 - thousand - 2.67\n",
      "10 - concerning - 2.64\n",
      "11 - introduced - 2.48\n",
      "12 - now - 2.47\n",
      "13 - circumstances - 2.46\n",
      "14 - duly - 2.35\n",
      "15 - ruled - 2.29\n",
      "16 - compare - 2.28\n",
      "17 - drawn - 2.26\n",
      "18 - judiciary - 2.23\n",
      "19 - decreed - 2.12\n",
      "20 - theory - 2.11\n",
      "\n",
      "Republican Top 20:\n",
      "1 - sect - -2.67\n",
      "2 - wall - -2.5\n",
      "3 - brought - -2.38\n",
      "4 - far - -2.37\n",
      "5 - id - -2.3\n",
      "6 - latter - -2.3\n",
      "7 - delivered - -2.19\n",
      "8 - shown - -2.15\n",
      "9 - simply - -2.11\n",
      "10 - testimony - -2.1\n",
      "11 - pursuance - -2.09\n",
      "12 - ruling - -2.08\n",
      "13 - 2d - -2.06\n",
      "14 - nothing - -2.03\n",
      "15 - consider - -2.02\n",
      "16 - accordance - -1.95\n",
      "17 - duty - -1.94\n",
      "18 - principle - -1.93\n",
      "19 - follows - -1.9\n",
      "20 - former - -1.87\n"
     ]
    }
   ],
   "source": [
    "# Find top 10 words for both Democrats and Republicans for L2 regularization\n",
    "coeff = clf_l2.coef_.flatten()\n",
    "democrat_top_20 = list(sorted(range(len(coeff)), key=lambda i: coeff[i], reverse=True)[0:20])\n",
    "republican_top_20 = list(sorted(range(len(coeff)), key=lambda i: coeff[i])[0:20])\n",
    "\n",
    "print(\"L2 results:\")\n",
    "\n",
    "print(\"Democrat Top 20:\")\n",
    "for i, d in enumerate(democrat_top_20):\n",
    "    print(i+1, '-', vocab.id_to_word.get(d), '-', round(coeff[d],2))\n",
    "\n",
    "print()\n",
    "print(\"Republican Top 20:\")\n",
    "for i, r in enumerate(republican_top_20):\n",
    "    print(i+1, '-', vocab.id_to_word.get(r), '-', round(coeff[r],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cross_entropy_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(1791.774, 1803.3]</th>\n",
       "      <td>1.272506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1803.3, 1814.6]</th>\n",
       "      <td>0.336828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1814.6, 1825.9]</th>\n",
       "      <td>0.445353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1825.9, 1837.2]</th>\n",
       "      <td>0.441889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1837.2, 1848.5]</th>\n",
       "      <td>0.658037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1848.5, 1859.8]</th>\n",
       "      <td>1.462013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1859.8, 1871.1]</th>\n",
       "      <td>1.897501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1871.1, 1882.4]</th>\n",
       "      <td>1.646741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1882.4, 1893.7]</th>\n",
       "      <td>1.429614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1893.7, 1905.0]</th>\n",
       "      <td>1.565920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1905.0, 1916.3]</th>\n",
       "      <td>1.350772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1916.3, 1927.6]</th>\n",
       "      <td>1.258802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1927.6, 1938.9]</th>\n",
       "      <td>0.835584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1938.9, 1950.2]</th>\n",
       "      <td>0.517797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1950.2, 1961.5]</th>\n",
       "      <td>0.712906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1961.5, 1972.8]</th>\n",
       "      <td>0.935882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1972.8, 1984.1]</th>\n",
       "      <td>1.261166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1984.1, 1995.4]</th>\n",
       "      <td>1.406597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1995.4, 2006.7]</th>\n",
       "      <td>1.379945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2006.7, 2018.0]</th>\n",
       "      <td>0.983069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    cross_entropy_loss\n",
       "(1791.774, 1803.3]            1.272506\n",
       "(1803.3, 1814.6]              0.336828\n",
       "(1814.6, 1825.9]              0.445353\n",
       "(1825.9, 1837.2]              0.441889\n",
       "(1837.2, 1848.5]              0.658037\n",
       "(1848.5, 1859.8]              1.462013\n",
       "(1859.8, 1871.1]              1.897501\n",
       "(1871.1, 1882.4]              1.646741\n",
       "(1882.4, 1893.7]              1.429614\n",
       "(1893.7, 1905.0]              1.565920\n",
       "(1905.0, 1916.3]              1.350772\n",
       "(1916.3, 1927.6]              1.258802\n",
       "(1927.6, 1938.9]              0.835584\n",
       "(1938.9, 1950.2]              0.517797\n",
       "(1950.2, 1961.5]              0.712906\n",
       "(1961.5, 1972.8]              0.935882\n",
       "(1972.8, 1984.1]              1.261166\n",
       "(1984.1, 1995.4]              1.406597\n",
       "(1995.4, 2006.7]              1.379945\n",
       "(2006.7, 2018.0]              0.983069"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get cross-entropy over time for L1 model\n",
    "out, _ = pd.cut([int(x) for x in train_data.year_filed], 20, retbins=True)\n",
    "bins = pd.cut([int(x) for x in train_data.year_filed], 22, labels=False)\n",
    "\n",
    "preds = np.log(clf_l1.predict_proba(train_x)[:,1])\n",
    "preds_df = pd.DataFrame({'bins': bins, 'log_prob': preds})\n",
    "\n",
    "agg_df = preds_df.groupby('bins').agg(lambda x: -np.mean(x.log_prob))\n",
    "agg_df.index = out.categories\n",
    "agg_df.columns = ['cross_entropy_loss']\n",
    "\n",
    "agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cross_entropy_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(1791.774, 1803.3]</th>\n",
       "      <td>0.944424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1803.3, 1814.6]</th>\n",
       "      <td>0.382964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1814.6, 1825.9]</th>\n",
       "      <td>0.525742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1825.9, 1837.2]</th>\n",
       "      <td>0.530336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1837.2, 1848.5]</th>\n",
       "      <td>0.641271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1848.5, 1859.8]</th>\n",
       "      <td>1.355997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1859.8, 1871.1]</th>\n",
       "      <td>1.692196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1871.1, 1882.4]</th>\n",
       "      <td>1.555249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1882.4, 1893.7]</th>\n",
       "      <td>1.343641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1893.7, 1905.0]</th>\n",
       "      <td>1.485937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1905.0, 1916.3]</th>\n",
       "      <td>1.259328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1916.3, 1927.6]</th>\n",
       "      <td>1.190373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1927.6, 1938.9]</th>\n",
       "      <td>0.774179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1938.9, 1950.2]</th>\n",
       "      <td>0.553427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1950.2, 1961.5]</th>\n",
       "      <td>0.726250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1961.5, 1972.8]</th>\n",
       "      <td>0.962657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1972.8, 1984.1]</th>\n",
       "      <td>1.224773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1984.1, 1995.4]</th>\n",
       "      <td>1.345455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1995.4, 2006.7]</th>\n",
       "      <td>1.346292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2006.7, 2018.0]</th>\n",
       "      <td>1.077301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    cross_entropy_loss\n",
       "(1791.774, 1803.3]            0.944424\n",
       "(1803.3, 1814.6]              0.382964\n",
       "(1814.6, 1825.9]              0.525742\n",
       "(1825.9, 1837.2]              0.530336\n",
       "(1837.2, 1848.5]              0.641271\n",
       "(1848.5, 1859.8]              1.355997\n",
       "(1859.8, 1871.1]              1.692196\n",
       "(1871.1, 1882.4]              1.555249\n",
       "(1882.4, 1893.7]              1.343641\n",
       "(1893.7, 1905.0]              1.485937\n",
       "(1905.0, 1916.3]              1.259328\n",
       "(1916.3, 1927.6]              1.190373\n",
       "(1927.6, 1938.9]              0.774179\n",
       "(1938.9, 1950.2]              0.553427\n",
       "(1950.2, 1961.5]              0.726250\n",
       "(1961.5, 1972.8]              0.962657\n",
       "(1972.8, 1984.1]              1.224773\n",
       "(1984.1, 1995.4]              1.345455\n",
       "(1995.4, 2006.7]              1.346292\n",
       "(2006.7, 2018.0]              1.077301"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get cross-entropy over time for L2 model\n",
    "out, _ = pd.cut([int(x) for x in train_data.year_filed], 20, retbins=True)\n",
    "bins = pd.cut([int(x) for x in train_data.year_filed], 22, labels=False)\n",
    "\n",
    "preds = np.log(clf_l2.predict_proba(train_x)[:,1])\n",
    "preds_df = pd.DataFrame({'bins': bins, 'log_prob': preds})\n",
    "\n",
    "agg_df = preds_df.groupby('bins').agg(lambda x: -np.mean(x.log_prob))\n",
    "agg_df.index = out.categories\n",
    "agg_df.columns = ['cross_entropy_loss']\n",
    "\n",
    "agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cross_entropy_loss</th>\n",
       "      <th>min_year</th>\n",
       "      <th>max_year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bins</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alito, Samuel</th>\n",
       "      <td>1.119271</td>\n",
       "      <td>2006</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baldwin, Henry</th>\n",
       "      <td>0.278859</td>\n",
       "      <td>1830</td>\n",
       "      <td>1844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Barbour, Philip</th>\n",
       "      <td>0.445166</td>\n",
       "      <td>1837</td>\n",
       "      <td>1840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Black, Hugo</th>\n",
       "      <td>0.591280</td>\n",
       "      <td>1937</td>\n",
       "      <td>1971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Blackmun, Harry</th>\n",
       "      <td>1.298135</td>\n",
       "      <td>1971</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Blatchford, Samuel</th>\n",
       "      <td>1.664280</td>\n",
       "      <td>1882</td>\n",
       "      <td>1893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bradley, Joseph</th>\n",
       "      <td>1.670991</td>\n",
       "      <td>1870</td>\n",
       "      <td>1891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brandeis, Louis</th>\n",
       "      <td>0.748530</td>\n",
       "      <td>1916</td>\n",
       "      <td>1939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brennan, William</th>\n",
       "      <td>1.071540</td>\n",
       "      <td>1956</td>\n",
       "      <td>1990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brewer, David</th>\n",
       "      <td>1.906523</td>\n",
       "      <td>1890</td>\n",
       "      <td>1910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Breyer, Stephen</th>\n",
       "      <td>0.968506</td>\n",
       "      <td>1995</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brown, Henry</th>\n",
       "      <td>1.695933</td>\n",
       "      <td>1891</td>\n",
       "      <td>1906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Burger, Warren</th>\n",
       "      <td>1.247779</td>\n",
       "      <td>1970</td>\n",
       "      <td>1986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Burton, Harold</th>\n",
       "      <td>0.615533</td>\n",
       "      <td>1946</td>\n",
       "      <td>1958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Butler, Pierce</th>\n",
       "      <td>1.451555</td>\n",
       "      <td>1923</td>\n",
       "      <td>1939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Byrnes, James</th>\n",
       "      <td>0.678135</td>\n",
       "      <td>1941</td>\n",
       "      <td>1942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Campbell, John</th>\n",
       "      <td>0.673797</td>\n",
       "      <td>1853</td>\n",
       "      <td>1861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cardozo, Benjamin</th>\n",
       "      <td>1.540081</td>\n",
       "      <td>1932</td>\n",
       "      <td>1937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Catron, John</th>\n",
       "      <td>0.470757</td>\n",
       "      <td>1838</td>\n",
       "      <td>1864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chase, Samuel</th>\n",
       "      <td>1.569488</td>\n",
       "      <td>1796</td>\n",
       "      <td>1801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clark, Tom</th>\n",
       "      <td>0.599058</td>\n",
       "      <td>1949</td>\n",
       "      <td>1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clarke, John</th>\n",
       "      <td>0.832840</td>\n",
       "      <td>1916</td>\n",
       "      <td>1922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clifford, Nathan</th>\n",
       "      <td>0.703818</td>\n",
       "      <td>1858</td>\n",
       "      <td>1880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Daniel, Peter</th>\n",
       "      <td>0.462613</td>\n",
       "      <td>1842</td>\n",
       "      <td>1859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Davis, David</th>\n",
       "      <td>1.721400</td>\n",
       "      <td>1864</td>\n",
       "      <td>1877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day, William</th>\n",
       "      <td>1.425455</td>\n",
       "      <td>1903</td>\n",
       "      <td>1922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Douglas, William</th>\n",
       "      <td>0.546276</td>\n",
       "      <td>1939</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ellsworth, Oliver</th>\n",
       "      <td>1.486438</td>\n",
       "      <td>1799</td>\n",
       "      <td>1799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Field, Stephen</th>\n",
       "      <td>1.891212</td>\n",
       "      <td>1864</td>\n",
       "      <td>1896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fortas, Abe</th>\n",
       "      <td>0.647984</td>\n",
       "      <td>1965</td>\n",
       "      <td>1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Powell, Lewis</th>\n",
       "      <td>1.258368</td>\n",
       "      <td>1972</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reed, Stanley</th>\n",
       "      <td>0.616735</td>\n",
       "      <td>1938</td>\n",
       "      <td>1957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rehnquist, William</th>\n",
       "      <td>1.379326</td>\n",
       "      <td>1972</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roberts, John</th>\n",
       "      <td>1.102876</td>\n",
       "      <td>2006</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roberts, Owen</th>\n",
       "      <td>1.087187</td>\n",
       "      <td>1930</td>\n",
       "      <td>1945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rutledge, Wiley</th>\n",
       "      <td>0.450220</td>\n",
       "      <td>1943</td>\n",
       "      <td>1949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sanford, Edward</th>\n",
       "      <td>1.297307</td>\n",
       "      <td>1923</td>\n",
       "      <td>1930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Scalia, Antonin</th>\n",
       "      <td>1.473391</td>\n",
       "      <td>1986</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shiras, George</th>\n",
       "      <td>1.491318</td>\n",
       "      <td>1892</td>\n",
       "      <td>1903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sotomayor, Sonia</th>\n",
       "      <td>0.772223</td>\n",
       "      <td>2010</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Souter, David</th>\n",
       "      <td>1.646435</td>\n",
       "      <td>1991</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stevens, John</th>\n",
       "      <td>1.454756</td>\n",
       "      <td>1976</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stewart, Potter</th>\n",
       "      <td>1.082778</td>\n",
       "      <td>1959</td>\n",
       "      <td>1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stone, Harlan</th>\n",
       "      <td>1.114853</td>\n",
       "      <td>1925</td>\n",
       "      <td>1946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Strong, William</th>\n",
       "      <td>1.699987</td>\n",
       "      <td>1870</td>\n",
       "      <td>1880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sutherland, George</th>\n",
       "      <td>1.504745</td>\n",
       "      <td>1922</td>\n",
       "      <td>1938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Swayne, Noah</th>\n",
       "      <td>1.853917</td>\n",
       "      <td>1863</td>\n",
       "      <td>1881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Taft, William</th>\n",
       "      <td>1.446859</td>\n",
       "      <td>1921</td>\n",
       "      <td>1929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Taney, Roger</th>\n",
       "      <td>0.581779</td>\n",
       "      <td>1837</td>\n",
       "      <td>1861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Thomas, Clarence</th>\n",
       "      <td>1.365194</td>\n",
       "      <td>1992</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vinson, Frederick</th>\n",
       "      <td>0.513196</td>\n",
       "      <td>1946</td>\n",
       "      <td>1953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Waite, Morrison</th>\n",
       "      <td>2.023507</td>\n",
       "      <td>1875</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Warren, Earl</th>\n",
       "      <td>0.786162</td>\n",
       "      <td>1953</td>\n",
       "      <td>1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wayne, James</th>\n",
       "      <td>0.651398</td>\n",
       "      <td>1835</td>\n",
       "      <td>1864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>White, Byron</th>\n",
       "      <td>0.972893</td>\n",
       "      <td>1962</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>White, Edward</th>\n",
       "      <td>1.191902</td>\n",
       "      <td>1894</td>\n",
       "      <td>1921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Whittaker, Charles</th>\n",
       "      <td>0.804189</td>\n",
       "      <td>1957</td>\n",
       "      <td>1962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wilson, James</th>\n",
       "      <td>1.208596</td>\n",
       "      <td>1793</td>\n",
       "      <td>1795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Woodbury, Levi</th>\n",
       "      <td>0.366188</td>\n",
       "      <td>1845</td>\n",
       "      <td>1851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Woods, William</th>\n",
       "      <td>2.009880</td>\n",
       "      <td>1881</td>\n",
       "      <td>1886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    cross_entropy_loss min_year max_year\n",
       "bins                                                    \n",
       "Alito, Samuel                 1.119271     2006     2018\n",
       "Baldwin, Henry                0.278859     1830     1844\n",
       "Barbour, Philip               0.445166     1837     1840\n",
       "Black, Hugo                   0.591280     1937     1971\n",
       "Blackmun, Harry               1.298135     1971     1994\n",
       "Blatchford, Samuel            1.664280     1882     1893\n",
       "Bradley, Joseph               1.670991     1870     1891\n",
       "Brandeis, Louis               0.748530     1916     1939\n",
       "Brennan, William              1.071540     1956     1990\n",
       "Brewer, David                 1.906523     1890     1910\n",
       "Breyer, Stephen               0.968506     1995     2018\n",
       "Brown, Henry                  1.695933     1891     1906\n",
       "Burger, Warren                1.247779     1970     1986\n",
       "Burton, Harold                0.615533     1946     1958\n",
       "Butler, Pierce                1.451555     1923     1939\n",
       "Byrnes, James                 0.678135     1941     1942\n",
       "Campbell, John                0.673797     1853     1861\n",
       "Cardozo, Benjamin             1.540081     1932     1937\n",
       "Catron, John                  0.470757     1838     1864\n",
       "Chase, Samuel                 1.569488     1796     1801\n",
       "Clark, Tom                    0.599058     1949     1967\n",
       "Clarke, John                  0.832840     1916     1922\n",
       "Clifford, Nathan              0.703818     1858     1880\n",
       "Daniel, Peter                 0.462613     1842     1859\n",
       "Davis, David                  1.721400     1864     1877\n",
       "Day, William                  1.425455     1903     1922\n",
       "Douglas, William              0.546276     1939     1975\n",
       "Ellsworth, Oliver             1.486438     1799     1799\n",
       "Field, Stephen                1.891212     1864     1896\n",
       "Fortas, Abe                   0.647984     1965     1969\n",
       "...                                ...      ...      ...\n",
       "Powell, Lewis                 1.258368     1972     1987\n",
       "Reed, Stanley                 0.616735     1938     1957\n",
       "Rehnquist, William            1.379326     1972     2005\n",
       "Roberts, John                 1.102876     2006     2018\n",
       "Roberts, Owen                 1.087187     1930     1945\n",
       "Rutledge, Wiley               0.450220     1943     1949\n",
       "Sanford, Edward               1.297307     1923     1930\n",
       "Scalia, Antonin               1.473391     1986     2009\n",
       "Shiras, George                1.491318     1892     1903\n",
       "Sotomayor, Sonia              0.772223     2010     2018\n",
       "Souter, David                 1.646435     1991     2008\n",
       "Stevens, John                 1.454756     1976     2009\n",
       "Stewart, Potter               1.082778     1959     1981\n",
       "Stone, Harlan                 1.114853     1925     1946\n",
       "Strong, William               1.699987     1870     1880\n",
       "Sutherland, George            1.504745     1922     1938\n",
       "Swayne, Noah                  1.853917     1863     1881\n",
       "Taft, William                 1.446859     1921     1929\n",
       "Taney, Roger                  0.581779     1837     1861\n",
       "Thomas, Clarence              1.365194     1992     2011\n",
       "Vinson, Frederick             0.513196     1946     1953\n",
       "Waite, Morrison               2.023507     1875     2010\n",
       "Warren, Earl                  0.786162     1953     1969\n",
       "Wayne, James                  0.651398     1835     1864\n",
       "White, Byron                  0.972893     1962     1993\n",
       "White, Edward                 1.191902     1894     1921\n",
       "Whittaker, Charles            0.804189     1957     1962\n",
       "Wilson, James                 1.208596     1793     1795\n",
       "Woodbury, Levi                0.366188     1845     1851\n",
       "Woods, William                2.009880     1881     1886\n",
       "\n",
       "[94 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get cross-entropy by judge for L1 model\n",
    "preds = np.log(clf_l1.predict_proba(train_x)[:,1])\n",
    "preds_df = pd.DataFrame({'bins': train_data.name_last+', '+train_data.name_first, 'log_prob': preds, 'year1': train_data.year_filed, 'year2': train_data.year_filed})\n",
    "\n",
    "agg_df = preds_df.groupby('bins').agg(lambda x: [-np.mean(x.log_prob), np.min(x.year1), np.max(x.year2)])\n",
    "agg_df.columns = ['cross_entropy_loss', 'min_year', 'max_year']\n",
    "\n",
    "agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cross_entropy_loss</th>\n",
       "      <th>min_year</th>\n",
       "      <th>max_year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bins</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alito, Samuel</th>\n",
       "      <td>1.279106</td>\n",
       "      <td>2006</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baldwin, Henry</th>\n",
       "      <td>0.381915</td>\n",
       "      <td>1830</td>\n",
       "      <td>1844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Barbour, Philip</th>\n",
       "      <td>0.641383</td>\n",
       "      <td>1837</td>\n",
       "      <td>1840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Black, Hugo</th>\n",
       "      <td>0.571154</td>\n",
       "      <td>1937</td>\n",
       "      <td>1971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Blackmun, Harry</th>\n",
       "      <td>1.301927</td>\n",
       "      <td>1971</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Blatchford, Samuel</th>\n",
       "      <td>1.598906</td>\n",
       "      <td>1882</td>\n",
       "      <td>1893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bradley, Joseph</th>\n",
       "      <td>1.572278</td>\n",
       "      <td>1870</td>\n",
       "      <td>1891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brandeis, Louis</th>\n",
       "      <td>0.741049</td>\n",
       "      <td>1916</td>\n",
       "      <td>1939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brennan, William</th>\n",
       "      <td>1.089571</td>\n",
       "      <td>1956</td>\n",
       "      <td>1990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brewer, David</th>\n",
       "      <td>1.745609</td>\n",
       "      <td>1890</td>\n",
       "      <td>1910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Breyer, Stephen</th>\n",
       "      <td>0.964161</td>\n",
       "      <td>1995</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brown, Henry</th>\n",
       "      <td>1.619663</td>\n",
       "      <td>1891</td>\n",
       "      <td>1906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Burger, Warren</th>\n",
       "      <td>1.273370</td>\n",
       "      <td>1970</td>\n",
       "      <td>1986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Burton, Harold</th>\n",
       "      <td>0.542487</td>\n",
       "      <td>1946</td>\n",
       "      <td>1958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Butler, Pierce</th>\n",
       "      <td>1.345763</td>\n",
       "      <td>1923</td>\n",
       "      <td>1939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Byrnes, James</th>\n",
       "      <td>0.598847</td>\n",
       "      <td>1941</td>\n",
       "      <td>1942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Campbell, John</th>\n",
       "      <td>0.663579</td>\n",
       "      <td>1853</td>\n",
       "      <td>1861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cardozo, Benjamin</th>\n",
       "      <td>1.354455</td>\n",
       "      <td>1932</td>\n",
       "      <td>1937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Catron, John</th>\n",
       "      <td>0.468119</td>\n",
       "      <td>1838</td>\n",
       "      <td>1864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chase, Samuel</th>\n",
       "      <td>1.091108</td>\n",
       "      <td>1796</td>\n",
       "      <td>1801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clark, Tom</th>\n",
       "      <td>0.611511</td>\n",
       "      <td>1949</td>\n",
       "      <td>1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clarke, John</th>\n",
       "      <td>0.873795</td>\n",
       "      <td>1916</td>\n",
       "      <td>1922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clifford, Nathan</th>\n",
       "      <td>0.721724</td>\n",
       "      <td>1858</td>\n",
       "      <td>1880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Daniel, Peter</th>\n",
       "      <td>0.583538</td>\n",
       "      <td>1842</td>\n",
       "      <td>1859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Davis, David</th>\n",
       "      <td>1.571483</td>\n",
       "      <td>1864</td>\n",
       "      <td>1877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Day, William</th>\n",
       "      <td>1.431881</td>\n",
       "      <td>1903</td>\n",
       "      <td>1922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Douglas, William</th>\n",
       "      <td>0.564008</td>\n",
       "      <td>1939</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ellsworth, Oliver</th>\n",
       "      <td>1.003781</td>\n",
       "      <td>1799</td>\n",
       "      <td>1799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Field, Stephen</th>\n",
       "      <td>1.692869</td>\n",
       "      <td>1864</td>\n",
       "      <td>1896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fortas, Abe</th>\n",
       "      <td>0.668241</td>\n",
       "      <td>1965</td>\n",
       "      <td>1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Powell, Lewis</th>\n",
       "      <td>1.301568</td>\n",
       "      <td>1972</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reed, Stanley</th>\n",
       "      <td>0.574567</td>\n",
       "      <td>1938</td>\n",
       "      <td>1957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rehnquist, William</th>\n",
       "      <td>1.344880</td>\n",
       "      <td>1972</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roberts, John</th>\n",
       "      <td>1.260942</td>\n",
       "      <td>2006</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roberts, Owen</th>\n",
       "      <td>1.082537</td>\n",
       "      <td>1930</td>\n",
       "      <td>1945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rutledge, Wiley</th>\n",
       "      <td>0.505522</td>\n",
       "      <td>1943</td>\n",
       "      <td>1949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sanford, Edward</th>\n",
       "      <td>1.299185</td>\n",
       "      <td>1923</td>\n",
       "      <td>1930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Scalia, Antonin</th>\n",
       "      <td>1.431629</td>\n",
       "      <td>1986</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shiras, George</th>\n",
       "      <td>1.479306</td>\n",
       "      <td>1892</td>\n",
       "      <td>1903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sotomayor, Sonia</th>\n",
       "      <td>0.745415</td>\n",
       "      <td>2010</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Souter, David</th>\n",
       "      <td>1.523649</td>\n",
       "      <td>1991</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stevens, John</th>\n",
       "      <td>1.439756</td>\n",
       "      <td>1976</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stewart, Potter</th>\n",
       "      <td>1.106000</td>\n",
       "      <td>1959</td>\n",
       "      <td>1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stone, Harlan</th>\n",
       "      <td>1.087350</td>\n",
       "      <td>1925</td>\n",
       "      <td>1946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Strong, William</th>\n",
       "      <td>1.636644</td>\n",
       "      <td>1870</td>\n",
       "      <td>1880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sutherland, George</th>\n",
       "      <td>1.425640</td>\n",
       "      <td>1922</td>\n",
       "      <td>1938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Swayne, Noah</th>\n",
       "      <td>1.634846</td>\n",
       "      <td>1863</td>\n",
       "      <td>1881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Taft, William</th>\n",
       "      <td>1.334891</td>\n",
       "      <td>1921</td>\n",
       "      <td>1929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Taney, Roger</th>\n",
       "      <td>0.608092</td>\n",
       "      <td>1837</td>\n",
       "      <td>1861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Thomas, Clarence</th>\n",
       "      <td>1.348500</td>\n",
       "      <td>1992</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vinson, Frederick</th>\n",
       "      <td>0.565219</td>\n",
       "      <td>1946</td>\n",
       "      <td>1953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Waite, Morrison</th>\n",
       "      <td>1.732324</td>\n",
       "      <td>1875</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Warren, Earl</th>\n",
       "      <td>0.885597</td>\n",
       "      <td>1953</td>\n",
       "      <td>1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wayne, James</th>\n",
       "      <td>0.652278</td>\n",
       "      <td>1835</td>\n",
       "      <td>1864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>White, Byron</th>\n",
       "      <td>0.885081</td>\n",
       "      <td>1962</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>White, Edward</th>\n",
       "      <td>1.134371</td>\n",
       "      <td>1894</td>\n",
       "      <td>1921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Whittaker, Charles</th>\n",
       "      <td>0.838022</td>\n",
       "      <td>1957</td>\n",
       "      <td>1962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wilson, James</th>\n",
       "      <td>0.953272</td>\n",
       "      <td>1793</td>\n",
       "      <td>1795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Woodbury, Levi</th>\n",
       "      <td>0.543713</td>\n",
       "      <td>1845</td>\n",
       "      <td>1851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Woods, William</th>\n",
       "      <td>1.780083</td>\n",
       "      <td>1881</td>\n",
       "      <td>1886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    cross_entropy_loss min_year max_year\n",
       "bins                                                    \n",
       "Alito, Samuel                 1.279106     2006     2018\n",
       "Baldwin, Henry                0.381915     1830     1844\n",
       "Barbour, Philip               0.641383     1837     1840\n",
       "Black, Hugo                   0.571154     1937     1971\n",
       "Blackmun, Harry               1.301927     1971     1994\n",
       "Blatchford, Samuel            1.598906     1882     1893\n",
       "Bradley, Joseph               1.572278     1870     1891\n",
       "Brandeis, Louis               0.741049     1916     1939\n",
       "Brennan, William              1.089571     1956     1990\n",
       "Brewer, David                 1.745609     1890     1910\n",
       "Breyer, Stephen               0.964161     1995     2018\n",
       "Brown, Henry                  1.619663     1891     1906\n",
       "Burger, Warren                1.273370     1970     1986\n",
       "Burton, Harold                0.542487     1946     1958\n",
       "Butler, Pierce                1.345763     1923     1939\n",
       "Byrnes, James                 0.598847     1941     1942\n",
       "Campbell, John                0.663579     1853     1861\n",
       "Cardozo, Benjamin             1.354455     1932     1937\n",
       "Catron, John                  0.468119     1838     1864\n",
       "Chase, Samuel                 1.091108     1796     1801\n",
       "Clark, Tom                    0.611511     1949     1967\n",
       "Clarke, John                  0.873795     1916     1922\n",
       "Clifford, Nathan              0.721724     1858     1880\n",
       "Daniel, Peter                 0.583538     1842     1859\n",
       "Davis, David                  1.571483     1864     1877\n",
       "Day, William                  1.431881     1903     1922\n",
       "Douglas, William              0.564008     1939     1975\n",
       "Ellsworth, Oliver             1.003781     1799     1799\n",
       "Field, Stephen                1.692869     1864     1896\n",
       "Fortas, Abe                   0.668241     1965     1969\n",
       "...                                ...      ...      ...\n",
       "Powell, Lewis                 1.301568     1972     1987\n",
       "Reed, Stanley                 0.574567     1938     1957\n",
       "Rehnquist, William            1.344880     1972     2005\n",
       "Roberts, John                 1.260942     2006     2018\n",
       "Roberts, Owen                 1.082537     1930     1945\n",
       "Rutledge, Wiley               0.505522     1943     1949\n",
       "Sanford, Edward               1.299185     1923     1930\n",
       "Scalia, Antonin               1.431629     1986     2009\n",
       "Shiras, George                1.479306     1892     1903\n",
       "Sotomayor, Sonia              0.745415     2010     2018\n",
       "Souter, David                 1.523649     1991     2008\n",
       "Stevens, John                 1.439756     1976     2009\n",
       "Stewart, Potter               1.106000     1959     1981\n",
       "Stone, Harlan                 1.087350     1925     1946\n",
       "Strong, William               1.636644     1870     1880\n",
       "Sutherland, George            1.425640     1922     1938\n",
       "Swayne, Noah                  1.634846     1863     1881\n",
       "Taft, William                 1.334891     1921     1929\n",
       "Taney, Roger                  0.608092     1837     1861\n",
       "Thomas, Clarence              1.348500     1992     2011\n",
       "Vinson, Frederick             0.565219     1946     1953\n",
       "Waite, Morrison               1.732324     1875     2010\n",
       "Warren, Earl                  0.885597     1953     1969\n",
       "Wayne, James                  0.652278     1835     1864\n",
       "White, Byron                  0.885081     1962     1993\n",
       "White, Edward                 1.134371     1894     1921\n",
       "Whittaker, Charles            0.838022     1957     1962\n",
       "Wilson, James                 0.953272     1793     1795\n",
       "Woodbury, Levi                0.543713     1845     1851\n",
       "Woods, William                1.780083     1881     1886\n",
       "\n",
       "[94 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get cross-entropy by judge for L2 model\n",
    "preds = np.log(clf_l2.predict_proba(train_x)[:,1])\n",
    "preds_df = pd.DataFrame({'bins': train_data.name_last+', '+train_data.name_first, 'log_prob': preds, 'year1': train_data.year_filed, 'year2': train_data.year_filed})\n",
    "\n",
    "agg_df = preds_df.groupby('bins').agg(lambda x: [-np.mean(x.log_prob), np.min(x.year1), np.max(x.year2)])\n",
    "agg_df.columns = ['cross_entropy_loss', 'min_year', 'max_year']\n",
    "\n",
    "agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Making stop word set smaller increased accuracy. Commonality of words is still taken care of by IDF, but allows for additional information if words are more commonly used by Dem / Rep. We will keep this modification going forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified Regression Parameters - L1 regularization followed by L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17052, 10000)\n",
      "(17052,)\n",
      "(2131, 10000)\n",
      "(2131,)\n",
      "(2133, 10000)\n",
      "(2133,)\n"
     ]
    }
   ],
   "source": [
    "# Define TF-IDF vectorizer; Using top 1000 words as stop words.\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "# Remove judge names b/c these are strong indicators of political party without indicating how language is used in opinions\n",
    "# Stop word set up to position 200, corresponds to 99.65 percentile with words having counts> 38746\n",
    "# Still includes common words like 'petitioners', but IDF weighting should take care of that \n",
    "stop_word_set = [w for w, i in vocab.unigram_counts.most_common(200)]\n",
    "stop_word_set.extend([judge_name for judge_name in set(w.lower() for w in data.name_last)])\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None,\n",
    "    vocabulary=vocab.word_to_id,\n",
    "    stop_words=stop_word_set)  \n",
    "\n",
    "# Transform to TFIDF data sets\n",
    "\n",
    "train_x = tfidf.fit_transform(train_data.text)\n",
    "train_y = train_data.y\n",
    "\n",
    "dev_x = tfidf.transform(dev_data.text)\n",
    "dev_y = dev_data.y\n",
    "\n",
    "test_x = tfidf.transform(test_data.text)\n",
    "test_y = test_data.y\n",
    "\n",
    "# print shapes for confirmation\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(dev_x.shape)\n",
    "print(dev_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 stats:\n",
      "0.789643443584\n",
      "0.763022055373\n",
      "L2 stats:\n",
      "0.809523809524\n",
      "0.756452369779\n"
     ]
    }
   ],
   "source": [
    "# Fit logitic regression (L1)\n",
    "clf_l1 = linear_model.LogisticRegression(penalty='l1')\n",
    "clf_l1.fit(train_x, train_y)\n",
    "\n",
    "print(\"L1 stats:\")\n",
    "print(clf_l1.score(train_x, train_y))\n",
    "print(clf_l1.score(dev_x, dev_y))\n",
    "\n",
    "# Fit logitic regression (L2)\n",
    "clf_l2 = linear_model.LogisticRegression(penalty='l2')\n",
    "clf_l2.fit(train_x, train_y)\n",
    "\n",
    "print(\"L2 stats:\")\n",
    "print(clf_l2.score(train_x, train_y))\n",
    "print(clf_l2.score(dev_x, dev_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9105 coefficients equal to 0\n",
      "There are 895 coefficients not equal to 0\n",
      "(17052, 895)\n",
      "(2131, 895)\n",
      "(2133, 895)\n"
     ]
    }
   ],
   "source": [
    "# Remove features with coeff of 0 from L1 regression\n",
    "print(\"There are %d coefficients equal to 0\" % np.sum(clf_l1.coef_==0))\n",
    "print(\"There are %d coefficients not equal to 0\" % np.sum(clf_l1.coef_!=0))\n",
    "\n",
    "keep_column = clf_l1.coef_[0] != 0\n",
    "\n",
    "train_reduced = train_x[:,keep_column]\n",
    "dev_reduced = dev_x[:,keep_column]\n",
    "test_reduced = test_x[:,keep_column]\n",
    "\n",
    "# confirm shapes\n",
    "print(train_reduced.shape)\n",
    "print(dev_reduced.shape)\n",
    "print(test_reduced.shape)\n",
    "\n",
    "# reduce vocabulary?\n",
    "# would have done if this model was better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 stats:\n",
      "0.763194933146\n",
      "0.740497419052\n"
     ]
    }
   ],
   "source": [
    "# Fit logitic regression (L2)\n",
    "clf_l2 = linear_model.LogisticRegression(penalty='l2')\n",
    "clf_l2.fit(train_reduced, train_y)\n",
    "\n",
    "print(\"L2 stats:\")\n",
    "print(clf_l2.score(train_reduced, train_y))\n",
    "print(clf_l2.score(dev_reduced, dev_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.663734459301\n",
      "0.661191928672\n",
      "\n",
      "0.2\n",
      "0.681562280084\n",
      "0.679023932426\n",
      "\n",
      "0.3\n",
      "0.700621627962\n",
      "0.699202252464\n",
      "\n",
      "0.4\n",
      "0.718977246071\n",
      "0.715626466448\n",
      "\n",
      "0.5\n",
      "0.734166080225\n",
      "0.729235100892\n",
      "\n",
      "0.6\n",
      "0.749296270232\n",
      "0.738620366025\n",
      "\n",
      "0.7\n",
      "0.760321369927\n",
      "0.746128578132\n",
      "\n",
      "0.8\n",
      "0.771581046212\n",
      "0.752229000469\n",
      "\n",
      "0.9\n",
      "0.78078817734\n",
      "0.76067573909\n",
      "\n",
      "1.0\n",
      "0.789643443584\n",
      "0.763022055373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try different hyperparameters\n",
    "for c in np.arange(0.1, 1.1, 0.1):\n",
    "    clf_l1 = linear_model.LogisticRegression(penalty='l1', C=c)\n",
    "    clf_l1.fit(train_x, train_y)\n",
    "\n",
    "    print(c)\n",
    "    print(clf_l1.score(train_x, train_y))\n",
    "    print(clf_l1.score(dev_x, dev_y))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.677339901478\n",
      "0.670107930549\n",
      "\n",
      "0.2\n",
      "0.7107084213\n",
      "0.694509619897\n",
      "\n",
      "0.3\n",
      "0.734459300962\n",
      "0.717503519474\n",
      "\n",
      "0.4\n",
      "0.753342716397\n",
      "0.728765837635\n",
      "\n",
      "0.5\n",
      "0.767417311752\n",
      "0.733458470202\n",
      "\n",
      "0.6\n",
      "0.778266479005\n",
      "0.742374472079\n",
      "\n",
      "0.7\n",
      "0.78776683087\n",
      "0.746128578132\n",
      "\n",
      "0.8\n",
      "0.79562514661\n",
      "0.748474894416\n",
      "\n",
      "0.9\n",
      "0.802603800141\n",
      "0.753636790239\n",
      "\n",
      "1.0\n",
      "0.809523809524\n",
      "0.756452369779\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try different hyperparameters\n",
    "for c in np.arange(0.1, 1.1, 0.1):\n",
    "    clf_l2 = linear_model.LogisticRegression(penalty='l2', C=c)\n",
    "    clf_l2.fit(train_x, train_y)\n",
    "\n",
    "    print(c)\n",
    "    print(clf_l2.score(train_x, train_y))\n",
    "    print(clf_l2.score(dev_x, dev_y))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.668484635233\n",
      "0.663538244955\n",
      "\n",
      "0.2\n",
      "0.688365001173\n",
      "0.678085405913\n",
      "\n",
      "0.3\n",
      "0.705254515599\n",
      "0.695917409667\n",
      "\n",
      "0.4\n",
      "0.718625381187\n",
      "0.710464570624\n",
      "\n",
      "0.5\n",
      "0.729709125029\n",
      "0.722196152041\n",
      "\n",
      "0.6\n",
      "0.739561341778\n",
      "0.726419521351\n",
      "\n",
      "0.7\n",
      "0.747419657518\n",
      "0.732050680432\n",
      "\n",
      "0.8\n",
      "0.753987802017\n",
      "0.735804786485\n",
      "\n",
      "0.9\n",
      "0.758503401361\n",
      "0.737681839512\n",
      "\n",
      "1.0\n",
      "0.763194933146\n",
      "0.740497419052\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try different hyperparameters\n",
    "for c in np.arange(0.1, 1.1, 0.1):\n",
    "    clf_l2 = linear_model.LogisticRegression(penalty='l2', C=c)\n",
    "    clf_l2.fit(train_reduced, train_y)\n",
    "\n",
    "    print(c)\n",
    "    print(clf_l2.score(train_reduced, train_y))\n",
    "    print(clf_l2.score(dev_reduced, dev_y))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1:\n",
      "0.789643443584\n",
      "0.743084857009\n",
      "L2:\n",
      "0.809523809524\n",
      "0.732301922175\n",
      "L2 post L1 feature selection:\n",
      "0.763194933146\n",
      "0.717768401313\n"
     ]
    }
   ],
   "source": [
    "# Final model for each with best parameters\n",
    "clf_l1 = linear_model.LogisticRegression(penalty='l1', C=1.0)\n",
    "clf_l1.fit(train_x, train_y)\n",
    "\n",
    "print(\"L1:\")\n",
    "print(clf_l1.score(train_x, train_y))\n",
    "print(clf_l1.score(test_x, test_y))\n",
    "\n",
    "clf_l2 = linear_model.LogisticRegression(penalty='l2', C=1.0)\n",
    "clf_l2.fit(train_x, train_y)\n",
    "\n",
    "print(\"L2:\")\n",
    "print(clf_l2.score(train_x, train_y))\n",
    "print(clf_l2.score(test_x, test_y))\n",
    "\n",
    "clf_l2_reduced = linear_model.LogisticRegression(penalty='l2', C=1.0)\n",
    "clf_l2_reduced.fit(train_reduced, train_y)\n",
    "\n",
    "print(\"L2 post L1 feature selection:\")\n",
    "print(clf_l2_reduced.score(train_reduced, train_y))\n",
    "print(clf_l2_reduced.score(test_reduced, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Using L1 as a feature selection was not effective and did not improve accuracy scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Similarity Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21316, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Define TF-IDF vectorizer; Using top 1000 words as stop words.\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "# Remove judge names b/c these are strong indicators of political party without indicating how language is used in opinions\n",
    "# Stop word set up to position 200, corresponds to 99.65 percentile with words having counts> 38746\n",
    "# Still includes common words like 'petitioners', but IDF weighting should take care of that \n",
    "stop_word_set = [w for w, i in vocab.unigram_counts.most_common(200)]\n",
    "stop_word_set.extend([judge_name for judge_name in set(w.lower() for w in data.name_last)])\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None,\n",
    "    vocabulary=vocab.word_to_id,\n",
    "    stop_words=stop_word_set)  \n",
    "\n",
    "# Transform all data to TFIDF data sets (not modeling and calculating error, just looking at doc sim measures)\n",
    "\n",
    "all_x = tfidf.fit_transform(df.text)\n",
    "\n",
    "# print shapes for confirmation\n",
    "print(all_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decade</th>\n",
       "      <th>dem_cos_sim</th>\n",
       "      <th>rep_cos_sim</th>\n",
       "      <th>cross_party_cos_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1920</td>\n",
       "      <td>0.050556</td>\n",
       "      <td>0.044428</td>\n",
       "      <td>0.044262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1800</td>\n",
       "      <td>0.269329</td>\n",
       "      <td>0.237059</td>\n",
       "      <td>0.051135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1930</td>\n",
       "      <td>0.054945</td>\n",
       "      <td>0.052312</td>\n",
       "      <td>0.050454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1940</td>\n",
       "      <td>0.056247</td>\n",
       "      <td>0.059768</td>\n",
       "      <td>0.052134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1950</td>\n",
       "      <td>0.057973</td>\n",
       "      <td>0.072282</td>\n",
       "      <td>0.060163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1830</td>\n",
       "      <td>0.104416</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1960</td>\n",
       "      <td>0.058263</td>\n",
       "      <td>0.069704</td>\n",
       "      <td>0.060898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1840</td>\n",
       "      <td>0.076523</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1970</td>\n",
       "      <td>0.065405</td>\n",
       "      <td>0.069145</td>\n",
       "      <td>0.064322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1850</td>\n",
       "      <td>0.068560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1980</td>\n",
       "      <td>0.065156</td>\n",
       "      <td>0.068055</td>\n",
       "      <td>0.063978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1860</td>\n",
       "      <td>0.064909</td>\n",
       "      <td>0.061123</td>\n",
       "      <td>0.054677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1990</td>\n",
       "      <td>0.068611</td>\n",
       "      <td>0.071860</td>\n",
       "      <td>0.065933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1870</td>\n",
       "      <td>0.080552</td>\n",
       "      <td>0.055119</td>\n",
       "      <td>0.054918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2000</td>\n",
       "      <td>0.072432</td>\n",
       "      <td>0.071042</td>\n",
       "      <td>0.065285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1880</td>\n",
       "      <td>0.059306</td>\n",
       "      <td>0.049151</td>\n",
       "      <td>0.049171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2010</td>\n",
       "      <td>0.093863</td>\n",
       "      <td>0.098959</td>\n",
       "      <td>0.070032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1890</td>\n",
       "      <td>0.056444</td>\n",
       "      <td>0.054472</td>\n",
       "      <td>0.052440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1900</td>\n",
       "      <td>0.051017</td>\n",
       "      <td>0.052101</td>\n",
       "      <td>0.048932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1910</td>\n",
       "      <td>0.050203</td>\n",
       "      <td>0.048438</td>\n",
       "      <td>0.045458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1790</td>\n",
       "      <td>0.343337</td>\n",
       "      <td>0.176476</td>\n",
       "      <td>0.141838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    decade  dem_cos_sim  rep_cos_sim  cross_party_cos_sim\n",
       "0     1920     0.050556     0.044428             0.044262\n",
       "1     1800     0.269329     0.237059             0.051135\n",
       "2     1930     0.054945     0.052312             0.050454\n",
       "3     1940     0.056247     0.059768             0.052134\n",
       "4     1950     0.057973     0.072282             0.060163\n",
       "5     1830     0.104416     0.000000             0.000000\n",
       "6     1960     0.058263     0.069704             0.060898\n",
       "7     1840     0.076523     0.000000             0.000000\n",
       "8     1970     0.065405     0.069145             0.064322\n",
       "9     1850     0.068560     0.000000             0.000000\n",
       "10    1980     0.065156     0.068055             0.063978\n",
       "11    1860     0.064909     0.061123             0.054677\n",
       "12    1990     0.068611     0.071860             0.065933\n",
       "13    1870     0.080552     0.055119             0.054918\n",
       "14    2000     0.072432     0.071042             0.065285\n",
       "15    1880     0.059306     0.049151             0.049171\n",
       "16    2010     0.093863     0.098959             0.070032\n",
       "17    1890     0.056444     0.054472             0.052440\n",
       "18    1900     0.051017     0.052101             0.048932\n",
       "19    1910     0.050203     0.048438             0.045458\n",
       "20    1790     0.343337     0.176476             0.141838"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get document similarity measures over time\n",
    "\n",
    "# note that linear_kernel is the same as cosine similarity when working on normalized vectors. TFIDF vectors are normalized vectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# rest index on dataframe to match indices in the TFIDF matrix\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Add a column for the decade for grouping documents\n",
    "df['time_bin'] = df['year_filed'].apply(lambda x: int(str(x[:3])+str(0)))\n",
    "unique_bins = list(set(df['time_bin']))\n",
    "\n",
    "# Get true/false with indices by party\n",
    "dem = df.loc[:,'y'] == 1\n",
    "rep = df.loc[:,'y'] == 0\n",
    "\n",
    "# create array to store results\n",
    "docsim_results = []\n",
    "\n",
    "# Loop through bins to calculate internal party and cross part average cosine similarity\n",
    "for bin in unique_bins:\n",
    "    # get indices of values in each party in this time period\n",
    "    time = df.loc[:,'time_bin'] == bin\n",
    "    dem_in_period = dem & time\n",
    "    rep_in_period = rep & time\n",
    "    dem_in_period_idx = dem_in_period[dem_in_period].index\n",
    "    rep_in_period_idx = rep_in_period[rep_in_period].index\n",
    "    \n",
    "    # get cosine similarity matrix - internal to each party and across parties\n",
    "    # then take mean to get average cos similarity\n",
    "    if len(dem_in_period_idx)>0:\n",
    "        dem_internal_cos_sim = np.mean(linear_kernel(all_x[dem_in_period_idx], all_x[dem_in_period_idx]))\n",
    "    else:\n",
    "        dem_internal_cos_sim = 0\n",
    "    if len(rep_in_period_idx)>0:\n",
    "        rep_internal_cos_sim = np.mean(linear_kernel(all_x[rep_in_period_idx], all_x[rep_in_period_idx]))\n",
    "    else:\n",
    "        rep_internal_cos_sim = 0\n",
    "    if len(rep_in_period_idx)>0 and len(dem_in_period_idx)>0:\n",
    "        cross_party_cos_sim = np.mean(linear_kernel(all_x[rep_in_period_idx], all_x[dem_in_period_idx]))\n",
    "    else:\n",
    "        cross_party_cos_sim = 0\n",
    "    \n",
    "    docsim_results.append((bin,dem_internal_cos_sim,rep_internal_cos_sim,cross_party_cos_sim))\n",
    "\n",
    "docsim_results = pd.DataFrame(docsim_results, columns=['decade', 'dem_cos_sim', 'rep_cos_sim', 'cross_party_cos_sim'])\n",
    "docsim_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
