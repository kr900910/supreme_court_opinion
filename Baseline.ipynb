{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from helpers import vocabulary\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and vocabulary preprocessed\n",
    "data = pickle.load(open(\"./data/data.p\", \"rb\"))\n",
    "vocab = pickle.load(open(\"./vocab.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define TF-IDF vectorizer; Using top 1000 words as stop words.\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None,\n",
    "    vocabulary=vocab.word_to_id,\n",
    "    stop_words=[w for w, i in vocab.unigram_counts.most_common(1000)])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out other political party\n",
    "df = data[(data.political_party=='r')|(data.political_party=='d')]\n",
    "\n",
    "# Shuffle data\n",
    "df = df.sample(frac=1)\n",
    "df.loc[:,'y'] = [1 if p == 'd' else 0 for p in df.political_party]\n",
    "\n",
    "# Divide data into train/dev/test\n",
    "train_number = int(len(df)*0.8)\n",
    "dev_number = int(len(df)*0.1)\n",
    "\n",
    "train_data = df[0:train_number]\n",
    "dev_data = df[train_number:train_number+dev_number]\n",
    "test_data = df[train_number+dev_number:]\n",
    "\n",
    "train_x = tfidf.fit_transform(train_data.text)\n",
    "train_y = train_data.y\n",
    "\n",
    "dev_x = tfidf.transform(dev_data.text)\n",
    "dev_y = dev_data.y\n",
    "\n",
    "test_x = tfidf.transform(test_data.text)\n",
    "test_y = test_data.y\n",
    "\n",
    "pickle.dump(train_data, open(\"train.p\", \"wb\"))\n",
    "pickle.dump(dev_data, open(\"dev.p\", \"wb\"))\n",
    "pickle.dump(test_data, open(\"test.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17052, 10000)\n",
      "(17052,)\n",
      "(2131, 10000)\n",
      "(2131,)\n",
      "(2133, 10000)\n",
      "(2133,)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(dev_x.shape)\n",
    "print(dev_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7995543044804129\n",
      "0.7705302674800563\n"
     ]
    }
   ],
   "source": [
    "# Fit logitic regression (L1)\n",
    "clf_l1 = linear_model.LogisticRegression(penalty='l1')\n",
    "clf_l1.fit(train_x, train_y)\n",
    "\n",
    "print(clf_l1.score(train_x, train_y))\n",
    "print(clf_l1.score(dev_x, dev_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8134529673938541\n",
      "0.7470671046457062\n"
     ]
    }
   ],
   "source": [
    "# Fit logitic regression (L2)\n",
    "clf_l2 = linear_model.LogisticRegression(penalty='l2')\n",
    "clf_l2.fit(train_x, train_y)\n",
    "\n",
    "print(clf_l2.score(train_x, train_y))\n",
    "print(clf_l2.score(dev_x, dev_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.6667839549612948\n",
      "0.669638667292351\n",
      "\n",
      "0.2\n",
      "0.6924700914848698\n",
      "0.6954481464101361\n",
      "\n",
      "0.30000000000000004\n",
      "0.7118813042458363\n",
      "0.7109338338808071\n",
      "\n",
      "0.4\n",
      "0.7300023457658925\n",
      "0.7297043641482872\n",
      "\n",
      "0.5\n",
      "0.7440769411212761\n",
      "0.7400281557954013\n",
      "\n",
      "0.6\n",
      "0.7565095003518649\n",
      "0.7433129985922102\n",
      "\n",
      "0.7000000000000001\n",
      "0.7691766361717101\n",
      "0.7541060534960112\n",
      "\n",
      "0.8\n",
      "0.7803190241613887\n",
      "0.7639605818864383\n",
      "\n",
      "0.9\n",
      "0.7902885292047853\n",
      "0.7672454246832473\n",
      "\n",
      "1.0\n",
      "0.7995543044804129\n",
      "0.7705302674800563\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try different hyperparameters\n",
    "for c in np.arange(0.1, 1.1, 0.1):\n",
    "    clf_l1 = linear_model.LogisticRegression(penalty='l1', C=c)\n",
    "    clf_l1.fit(train_x, train_y)\n",
    "\n",
    "    print(c)\n",
    "    print(clf_l1.score(train_x, train_y))\n",
    "    print(clf_l1.score(dev_x, dev_y))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7995543044804129\n",
      "0.7477730895452415\n"
     ]
    }
   ],
   "source": [
    "# Final model with L1 regularization with C = 1.0\n",
    "clf_l1 = linear_model.LogisticRegression(penalty='l1', C=1.0)\n",
    "clf_l1.fit(train_x, train_y)\n",
    "\n",
    "print(clf_l1.score(train_x, train_y))\n",
    "print(clf_l1.score(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Democrat Top 10:\n",
      "1 - frankfurter\n",
      "2 - fuller\n",
      "3 - whereof\n",
      "4 - problem\n",
      "5 - insofar\n",
      "6 - compare\n",
      "7 - douglas\n",
      "8 - furthermore\n",
      "9 - exhibited\n",
      "10 - consequently\n",
      "\n",
      "Republican Top 10:\n",
      "1 - brennan\n",
      "2 - waite\n",
      "3 - holmes\n",
      "4 - sutherland\n",
      "5 - stevens\n",
      "6 - isso\n",
      "7 - pursuance\n",
      "8 - matthews\n",
      "9 - brewer\n",
      "10 - observed\n"
     ]
    }
   ],
   "source": [
    "# Find top 10 words for both Democrats and Republicans\n",
    "coeff = clf_l1.coef_.flatten()\n",
    "democrat_top_10 = list(sorted(range(len(coeff)), key=lambda i: coeff[i], reverse=True)[0:10])\n",
    "republican_top_10 = list(sorted(range(len(coeff)), key=lambda i: coeff[i])[0:10])\n",
    "\n",
    "print(\"Democrat Top 10:\")\n",
    "for i, d in enumerate(democrat_top_10):\n",
    "    print(i+1, '-', vocab.id_to_word.get(d))\n",
    "\n",
    "print()\n",
    "print(\"Republican Top 10:\")\n",
    "for i, r in enumerate(republican_top_10):\n",
    "    print(i+1, '-', vocab.id_to_word.get(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cross_entropy_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(1791.774, 1814.6]</th>\n",
       "      <td>1.058688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1814.6, 1837.2]</th>\n",
       "      <td>0.526999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1837.2, 1859.8]</th>\n",
       "      <td>0.518075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1859.8, 1882.4]</th>\n",
       "      <td>1.821959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1882.4, 1905.0]</th>\n",
       "      <td>1.644657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1905.0, 1927.6]</th>\n",
       "      <td>1.507513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1927.6, 1950.2]</th>\n",
       "      <td>0.964998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1950.2, 1972.8]</th>\n",
       "      <td>0.771208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1972.8, 1995.4]</th>\n",
       "      <td>1.315938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1995.4, 2018.0]</th>\n",
       "      <td>1.225883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    cross_entropy_loss\n",
       "(1791.774, 1814.6]            1.058688\n",
       "(1814.6, 1837.2]              0.526999\n",
       "(1837.2, 1859.8]              0.518075\n",
       "(1859.8, 1882.4]              1.821959\n",
       "(1882.4, 1905.0]              1.644657\n",
       "(1905.0, 1927.6]              1.507513\n",
       "(1927.6, 1950.2]              0.964998\n",
       "(1950.2, 1972.8]              0.771208\n",
       "(1972.8, 1995.4]              1.315938\n",
       "(1995.4, 2018.0]              1.225883"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, _ = pd.cut([int(x) for x in train_data.year_filed], 10, retbins=True)\n",
    "bins = pd.cut([int(x) for x in train_data.year_filed], 10, labels=False)\n",
    "\n",
    "preds = np.log(clf_l1.predict_proba(train_x)[:,1])\n",
    "preds_df = pd.DataFrame({'bins': bins, 'log_prob': preds})\n",
    "\n",
    "agg_df = preds_df.groupby('bins').agg(lambda x: -np.mean(x.log_prob))\n",
    "agg_df.index = out.categories\n",
    "agg_df.columns = ['cross_entropy_loss']\n",
    "\n",
    "agg_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
