{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Model (CNNRNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mr_ham/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import gensim\n",
    "import os, shutil, time\n",
    "import model.CNNRNN as CNNRNN\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mr_ham/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Load word2vec pre-trained model\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "word2vec_matrix = word2vec_model.syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add <UNK> and <PAD> vectors to word2vec matrix\n",
    "unk_vec = np.random.uniform(-0.25,0.25,300) \n",
    "pad_vec = np.zeros(300) \n",
    "\n",
    "word2vec_matrix = np.vstack((word2vec_matrix, unk_vec))\n",
    "word2vec_matrix = np.vstack((word2vec_matrix, pad_vec))\n",
    "\n",
    "word2vec_words = [k for k in word2vec_model.vocab.keys()]\n",
    "word2vec_words = word2vec_words + ['<unk>', '<pad>']\n",
    "\n",
    "id_to_word = dict(enumerate(word2vec_words))\n",
    "word_to_id = {v:k for k,v in id_to_word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_generator(data, max_sent, max_word):\n",
    "    \n",
    "    # This function takes the input data in tokens and converted into matrix with indices\n",
    "    \n",
    "    inputs_x = np.zeros([len(data), max_sent, max_word])\n",
    "    inputs_y = np.zeros([len(data), 2])\n",
    "    num_doc = 0\n",
    "    \n",
    "    for i in data.index:\n",
    "        inputs_y[num_doc] = [0, 1] if data.loc[i, 'y'] == 1 else [1, 0]\n",
    "        \n",
    "        inputs_x_sent = np.zeros([max_sent, max_word])\n",
    "        inputs_x_sent.fill(3000001)\n",
    "        \n",
    "        sents = ' '.join(data.loc[i, 'text']).split('</s>')\n",
    "        num_sent = 0\n",
    "        \n",
    "        for s in sents:\n",
    "            if num_sent < max_sent:\n",
    "                if len(s) > 0:\n",
    "                    my_sent = s.rstrip().lstrip().split()\n",
    "                    num_word = 0\n",
    "                    for w in my_sent:\n",
    "                        if num_word < max_word:\n",
    "                            inputs_x_sent[num_sent, num_word] = word_to_id.get(w)\n",
    "                            num_word += 1\n",
    "                        else:\n",
    "                            break\n",
    "\n",
    "                    num_sent += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        inputs_x[num_doc] = inputs_x_sent\n",
    "        \n",
    "        num_doc += 1\n",
    "    \n",
    "    return inputs_x, inputs_y\n",
    "\n",
    "def batch_generator(inputs_x, inputs_y, batch_size):\n",
    "    num_batch = (len(inputs_y)) // batch_size\n",
    "    for i in range(num_batch):\n",
    "        yield inputs_x[i*batch_size:(i+1)*batch_size], inputs_y[i*batch_size:(i+1)*batch_size]\n",
    "        \n",
    "def score_dataset(lm, session, inputs_x, inputs_y, name=\"Train\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    bi = batch_generator(inputs_x, inputs_y, batch_size=1000)\n",
    "    cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=0.0, train=False, \n",
    "                     verbose=False, tick_s=3600)\n",
    "    print(\"{:s}: avg. loss: {:.03f}  (perplexity: {:.02f})\".format(name, cost, np.exp(cost)))\n",
    "    return cost\n",
    "\n",
    "def pretty_timedelta(fmt=\"%d:%02d:%02d\", since=None, until=None):\n",
    "    \"\"\"Pretty-print a timedelta, using the given format string.\"\"\"\n",
    "    since = since or time.time()\n",
    "    until = until or time.time()\n",
    "    delta_s = until - since\n",
    "    hours, remainder = divmod(delta_s, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return fmt % (hours, minutes, seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data = pickle.load(open(\"train.p\", \"rb\"))\n",
    "dev_data = pickle.load(open(\"dev.p\", \"rb\"))\n",
    "test_data = pickle.load(open(\"test.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the maximum sentence length per doc \n",
    "# and maximum number of words per sentence\n",
    "\n",
    "max_sent_length = 200\n",
    "sequence_length = 100\n",
    "\n",
    "train_x, train_y = input_generator(train_data, max_sent_length, sequence_length)\n",
    "dev_x, dev_y = input_generator(dev_data, max_sent_length, sequence_length)\n",
    "test_x, test_y = input_generator(test_data, max_sent_length, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17052, 200, 100)\n",
      "(17052, 2)\n",
      "(2131, 200, 100)\n",
      "(2131, 2)\n",
      "(2133, 200, 100)\n",
      "(2133, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(dev_x.shape)\n",
    "print(dev_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=None):\n",
    "    assert(learning_rate is not None)\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_accuracy = 0.0\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "    \n",
    "    loss = lm.loss_ \n",
    "    accuracy = lm.accuracy_\n",
    "\n",
    "    for i, (x, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        pred_accuracy = 0.0\n",
    "        \n",
    "        feed_dict = {lm.input_x_ : x,\n",
    "                     lm.input_y_ : y,\n",
    "                     lm.learning_rate_ : learning_rate}\n",
    "        \n",
    "        cost, pred_accuracy, _ = session.run([loss, accuracy, train_op], feed_dict=feed_dict)\n",
    "\n",
    "        total_cost += cost\n",
    "        total_accuracy += pred_accuracy\n",
    "        total_batches = i + 1\n",
    "\n",
    "        # Print average loss-so-far for epoch\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_accuracy = total_accuracy / total_batches\n",
    "            print(\"[batch {:d}]: loss = {:.3f}, accuracy = {:.3f}\".format(i, avg_cost, avg_accuracy))\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "max_time = 25\n",
    "batch_size = 200\n",
    "learning_rate = 0.01\n",
    "num_epochs = 5\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(max_sent_length=train_x.shape[1],\n",
    "                    batch_size=batch_size,\n",
    "                    num_classes=2,\n",
    "                    sequence_length=train_x.shape[2], \n",
    "                    vocab_size=word2vec_matrix.shape[0],\n",
    "                    embedding_size=word2vec_matrix.shape[1],\n",
    "                    filter_sizes=[1, 2, 3],\n",
    "                    conv_output_len=50,\n",
    "                    num_rnn_units=128)\n",
    "\n",
    "TF_SAVEDIR = \"./model/temp\"\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"model\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"model_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Starting epoch 1\n",
      "[batch 0]: loss = 1.325, accuracy = 0.435\n",
      "[batch 1]: loss = 5.842, accuracy = 0.540\n",
      "[batch 2]: loss = 4.467, accuracy = 0.603\n",
      "[batch 3]: loss = 4.698, accuracy = 0.544\n",
      "[batch 4]: loss = 3.990, accuracy = 0.536\n",
      "[batch 5]: loss = 3.604, accuracy = 0.551\n",
      "[batch 6]: loss = 3.253, accuracy = 0.569\n",
      "[batch 7]: loss = 2.949, accuracy = 0.569\n",
      "[batch 8]: loss = 2.788, accuracy = 0.541\n",
      "[batch 9]: loss = 2.597, accuracy = 0.534\n",
      "[batch 10]: loss = 2.443, accuracy = 0.545\n",
      "[batch 11]: loss = 2.320, accuracy = 0.556\n",
      "[batch 12]: loss = 2.232, accuracy = 0.559\n",
      "[batch 13]: loss = 2.137, accuracy = 0.563\n",
      "[batch 14]: loss = 2.046, accuracy = 0.560\n",
      "[batch 15]: loss = 1.990, accuracy = 0.550\n",
      "[batch 16]: loss = 1.929, accuracy = 0.539\n",
      "[batch 17]: loss = 1.862, accuracy = 0.541\n",
      "[batch 18]: loss = 1.804, accuracy = 0.546\n",
      "[batch 19]: loss = 1.757, accuracy = 0.552\n",
      "[batch 20]: loss = 1.715, accuracy = 0.556\n",
      "[batch 21]: loss = 1.679, accuracy = 0.558\n",
      "[batch 22]: loss = 1.634, accuracy = 0.564\n",
      "[batch 23]: loss = 1.597, accuracy = 0.566\n",
      "[batch 24]: loss = 1.563, accuracy = 0.565\n",
      "[batch 25]: loss = 1.532, accuracy = 0.564\n",
      "[batch 26]: loss = 1.503, accuracy = 0.565\n",
      "[batch 27]: loss = 1.475, accuracy = 0.567\n",
      "[batch 28]: loss = 1.446, accuracy = 0.570\n",
      "[batch 29]: loss = 1.419, accuracy = 0.574\n",
      "[batch 30]: loss = 1.397, accuracy = 0.577\n",
      "[batch 31]: loss = 1.376, accuracy = 0.580\n",
      "[batch 32]: loss = 1.356, accuracy = 0.583\n",
      "[batch 33]: loss = 1.337, accuracy = 0.585\n",
      "[batch 34]: loss = 1.319, accuracy = 0.587\n",
      "[batch 35]: loss = 1.301, accuracy = 0.587\n",
      "[batch 36]: loss = 1.285, accuracy = 0.587\n",
      "[batch 37]: loss = 1.268, accuracy = 0.589\n",
      "[batch 38]: loss = 1.254, accuracy = 0.588\n",
      "[batch 39]: loss = 1.240, accuracy = 0.588\n",
      "[batch 40]: loss = 1.226, accuracy = 0.589\n",
      "[batch 41]: loss = 1.212, accuracy = 0.591\n",
      "[batch 42]: loss = 1.199, accuracy = 0.593\n",
      "[batch 43]: loss = 1.186, accuracy = 0.595\n",
      "[batch 44]: loss = 1.175, accuracy = 0.596\n",
      "[batch 45]: loss = 1.164, accuracy = 0.598\n",
      "[batch 46]: loss = 1.153, accuracy = 0.600\n",
      "[batch 47]: loss = 1.146, accuracy = 0.599\n",
      "[batch 48]: loss = 1.136, accuracy = 0.601\n",
      "[batch 49]: loss = 1.125, accuracy = 0.603\n",
      "[batch 50]: loss = 1.116, accuracy = 0.603\n",
      "[batch 51]: loss = 1.108, accuracy = 0.604\n",
      "[batch 52]: loss = 1.099, accuracy = 0.604\n",
      "[batch 53]: loss = 1.090, accuracy = 0.606\n",
      "[batch 54]: loss = 1.082, accuracy = 0.607\n",
      "[batch 55]: loss = 1.074, accuracy = 0.608\n",
      "[batch 56]: loss = 1.067, accuracy = 0.609\n",
      "[batch 57]: loss = 1.060, accuracy = 0.609\n",
      "[batch 58]: loss = 1.053, accuracy = 0.611\n",
      "[batch 59]: loss = 1.046, accuracy = 0.612\n",
      "[batch 60]: loss = 1.039, accuracy = 0.613\n",
      "[batch 61]: loss = 1.033, accuracy = 0.613\n",
      "[batch 62]: loss = 1.028, accuracy = 0.614\n",
      "[batch 63]: loss = 1.022, accuracy = 0.614\n",
      "[batch 64]: loss = 1.015, accuracy = 0.616\n",
      "[batch 65]: loss = 1.009, accuracy = 0.618\n",
      "[batch 66]: loss = 1.004, accuracy = 0.617\n",
      "[batch 67]: loss = 0.999, accuracy = 0.618\n",
      "[batch 68]: loss = 0.994, accuracy = 0.618\n",
      "[batch 69]: loss = 0.990, accuracy = 0.619\n",
      "[batch 70]: loss = 0.985, accuracy = 0.619\n",
      "[batch 71]: loss = 0.980, accuracy = 0.620\n",
      "[batch 72]: loss = 0.976, accuracy = 0.620\n",
      "[batch 73]: loss = 0.971, accuracy = 0.621\n",
      "[batch 74]: loss = 0.968, accuracy = 0.621\n",
      "[batch 75]: loss = 0.964, accuracy = 0.620\n",
      "[batch 76]: loss = 0.960, accuracy = 0.621\n",
      "[batch 77]: loss = 0.956, accuracy = 0.622\n",
      "[batch 78]: loss = 0.952, accuracy = 0.622\n",
      "[batch 79]: loss = 0.949, accuracy = 0.622\n",
      "[batch 80]: loss = 0.945, accuracy = 0.624\n",
      "[batch 81]: loss = 0.941, accuracy = 0.624\n",
      "[batch 82]: loss = 0.938, accuracy = 0.624\n",
      "[batch 83]: loss = 0.934, accuracy = 0.624\n",
      "[batch 84]: loss = 0.931, accuracy = 0.625\n",
      "[epoch 1] Completed in 0:22:15\n",
      "[epoch 2] Starting epoch 2\n",
      "[batch 0]: loss = 0.654, accuracy = 0.660\n",
      "[batch 1]: loss = 0.675, accuracy = 0.638\n",
      "[batch 2]: loss = 0.640, accuracy = 0.668\n",
      "[batch 3]: loss = 0.648, accuracy = 0.661\n",
      "[batch 4]: loss = 0.641, accuracy = 0.663\n",
      "[batch 5]: loss = 0.644, accuracy = 0.657\n",
      "[batch 6]: loss = 0.639, accuracy = 0.659\n",
      "[batch 7]: loss = 0.653, accuracy = 0.653\n",
      "[batch 8]: loss = 0.661, accuracy = 0.648\n",
      "[batch 9]: loss = 0.666, accuracy = 0.644\n",
      "[batch 10]: loss = 0.669, accuracy = 0.643\n",
      "[batch 11]: loss = 0.669, accuracy = 0.640\n",
      "[batch 12]: loss = 0.670, accuracy = 0.639\n",
      "[batch 13]: loss = 0.670, accuracy = 0.636\n",
      "[batch 14]: loss = 0.670, accuracy = 0.638\n",
      "[batch 15]: loss = 0.670, accuracy = 0.640\n",
      "[batch 16]: loss = 0.668, accuracy = 0.641\n",
      "[batch 17]: loss = 0.665, accuracy = 0.643\n",
      "[batch 18]: loss = 0.669, accuracy = 0.641\n",
      "[batch 19]: loss = 0.673, accuracy = 0.640\n",
      "[batch 20]: loss = 0.674, accuracy = 0.638\n",
      "[batch 21]: loss = 0.678, accuracy = 0.636\n",
      "[batch 22]: loss = 0.675, accuracy = 0.640\n",
      "[batch 23]: loss = 0.675, accuracy = 0.640\n",
      "[batch 24]: loss = 0.677, accuracy = 0.639\n",
      "[batch 25]: loss = 0.676, accuracy = 0.640\n",
      "[batch 26]: loss = 0.675, accuracy = 0.640\n",
      "[batch 27]: loss = 0.674, accuracy = 0.641\n",
      "[batch 28]: loss = 0.672, accuracy = 0.642\n",
      "[batch 29]: loss = 0.672, accuracy = 0.642\n",
      "[batch 30]: loss = 0.670, accuracy = 0.643\n",
      "[batch 31]: loss = 0.669, accuracy = 0.644\n",
      "[batch 32]: loss = 0.669, accuracy = 0.643\n",
      "[batch 33]: loss = 0.669, accuracy = 0.643\n",
      "[batch 34]: loss = 0.670, accuracy = 0.643\n",
      "[batch 35]: loss = 0.670, accuracy = 0.643\n",
      "[batch 36]: loss = 0.670, accuracy = 0.643\n",
      "[batch 37]: loss = 0.668, accuracy = 0.644\n",
      "[batch 38]: loss = 0.668, accuracy = 0.644\n",
      "[batch 39]: loss = 0.667, accuracy = 0.644\n",
      "[batch 40]: loss = 0.669, accuracy = 0.642\n",
      "[batch 41]: loss = 0.669, accuracy = 0.642\n",
      "[batch 42]: loss = 0.669, accuracy = 0.642\n"
     ]
    }
   ],
   "source": [
    "reload(CNNRNN)\n",
    "\n",
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "\n",
    "lm = CNNRNN.CNNRNN(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "# shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer, feed_dict={lm.embedding_: word2vec_matrix})\n",
    "    \n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        bi = batch_generator(train_x, train_y, batch_size)\n",
    "        \n",
    "        print(\"[epoch {:d}] Starting epoch {:d}\".format(epoch, epoch))\n",
    "\n",
    "        # Run a training epoch.\n",
    "        run_epoch(lm, session, bi, train=True, verbose=True, tick_s=10, learning_rate=learning_rate)\n",
    "        \n",
    "        print(\"[epoch {:d}] Completed in {:s}\".format(epoch, pretty_timedelta(since=t0_epoch)))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)\n",
    "    \n",
    "    score_dataset(lm, session, train_x, train_y, 'Train Set')\n",
    "    score_dataset(lm, session, dev_x, dev_y, 'Dev Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dataset(lm, session, test_x, test_y, name=\"Test set\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
