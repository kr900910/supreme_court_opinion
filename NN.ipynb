{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Model (CNNRNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import gensim\n",
    "import os, shutil, time\n",
    "import model.CNNRNN as CNNRNN\n",
    "from importlib import reload\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mr_ham/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Load word2vec pre-trained model\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "word2vec_matrix = word2vec_model.syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add <UNK> and <PAD> vectors to word2vec matrix\n",
    "unk_vec = np.random.uniform(-0.25,0.25,300) \n",
    "pad_vec = np.zeros(300) \n",
    "\n",
    "word2vec_matrix = np.vstack((word2vec_matrix, unk_vec))\n",
    "word2vec_matrix = np.vstack((word2vec_matrix, pad_vec))\n",
    "\n",
    "word2vec_words = [k for k in word2vec_model.vocab.keys()]\n",
    "word2vec_words = word2vec_words + ['<unk>', '<pad>']\n",
    "\n",
    "id_to_word = dict(enumerate(word2vec_words))\n",
    "word_to_id = {v:k for k,v in id_to_word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_generator(data, max_sent, max_word):\n",
    "    \n",
    "    # This function takes the input data in tokens and converted into matrix with indices\n",
    "    \n",
    "    inputs_x = np.zeros([len(data), max_sent, max_word])\n",
    "    inputs_y = np.zeros([len(data), 2])\n",
    "    num_doc = 0\n",
    "    \n",
    "    for i in data.index:\n",
    "        inputs_y[num_doc] = [0, 1] if data.loc[i, 'y'] == 1 else [1, 0]\n",
    "        \n",
    "        inputs_x_sent = np.zeros([max_sent, max_word])\n",
    "        inputs_x_sent.fill(3000001)\n",
    "        \n",
    "        sents = ' '.join(data.loc[i, 'text']).split('</s>')\n",
    "        num_sent = 0\n",
    "        \n",
    "        for s in sents:\n",
    "            if num_sent < max_sent:\n",
    "                if len(s) > 0:\n",
    "                    my_sent = s.rstrip().lstrip().split()\n",
    "                    num_word = 0\n",
    "                    for w in my_sent:\n",
    "                        if num_word < max_word:\n",
    "                            inputs_x_sent[num_sent, num_word] = word_to_id.get(w)\n",
    "                            num_word += 1\n",
    "                        else:\n",
    "                            break\n",
    "\n",
    "                    num_sent += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        inputs_x[num_doc] = inputs_x_sent\n",
    "        \n",
    "        num_doc += 1\n",
    "    \n",
    "    return inputs_x, inputs_y\n",
    "\n",
    "def batch_generator(inputs_x, inputs_y, batch_size):\n",
    "    num_batch = (len(inputs_y)) // batch_size\n",
    "    for i in range(num_batch):\n",
    "        yield inputs_x[i*batch_size:(i+1)*batch_size], inputs_y[i*batch_size:(i+1)*batch_size]\n",
    "        \n",
    "def score_dataset(lm, session, inputs_x, inputs_y, batch_size, name=\"Train\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    bi = batch_generator(inputs_x, inputs_y, batch_size)\n",
    "    cost, pred_accuracy = run_epoch(lm, session, bi, \n",
    "                                    learning_rate=0.0, train=False, \n",
    "                                    verbose=True, tick_s=60)\n",
    "    print(\"{:s}: avg. loss: {:.03f}  (perplexity: {:.02f}), accuracy: {:.03f}\".format(name, cost, np.exp(cost), pred_accuracy))\n",
    "    return cost, pred_accuracy\n",
    "\n",
    "def pretty_timedelta(fmt=\"%d:%02d:%02d\", since=None, until=None):\n",
    "    \"\"\"Pretty-print a timedelta, using the given format string.\"\"\"\n",
    "    since = since or time.time()\n",
    "    until = until or time.time()\n",
    "    delta_s = until - since\n",
    "    hours, remainder = divmod(delta_s, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return fmt % (hours, minutes, seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data = pickle.load(open(\"train.p\", \"rb\"))\n",
    "dev_data = pickle.load(open(\"dev.p\", \"rb\"))\n",
    "test_data = pickle.load(open(\"test.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the maximum sentence length per doc \n",
    "# and maximum number of words per sentence\n",
    "\n",
    "max_sent_length = 200\n",
    "sequence_length = 100\n",
    "\n",
    "train_x, train_y = input_generator(train_data, max_sent_length, sequence_length)\n",
    "dev_x, dev_y = input_generator(dev_data, max_sent_length, sequence_length)\n",
    "test_x, test_y = input_generator(test_data, max_sent_length, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17052, 200, 100)\n",
      "(17052, 2)\n",
      "(2131, 200, 100)\n",
      "(2131, 2)\n",
      "(2133, 200, 100)\n",
      "(2133, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(dev_x.shape)\n",
    "print(dev_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=None):\n",
    "    \n",
    "    assert(learning_rate is not None)\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_accuracy = 0.0\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "    \n",
    "    loss = lm.loss_ \n",
    "    accuracy = lm.accuracy_\n",
    "\n",
    "    for i, (x, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        pred_accuracy = 0.0\n",
    "        \n",
    "        feed_dict = {lm.input_x_ : x,\n",
    "                     lm.input_y_ : y,\n",
    "                     lm.learning_rate_ : learning_rate}\n",
    "        \n",
    "        cost, pred_accuracy, _ = session.run([loss, accuracy, train_op], feed_dict=feed_dict)\n",
    "\n",
    "        total_cost += cost\n",
    "        total_accuracy += pred_accuracy\n",
    "        total_batches = i + 1\n",
    "\n",
    "        # Print average loss-so-far for epoch\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_accuracy = total_accuracy / total_batches\n",
    "            print(\"[batch {:d}]: loss = {:.3f}, accuracy = {:.3f}\".format(i, avg_cost, avg_accuracy))\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "        \n",
    "    return (total_cost / total_batches, total_accuracy / total_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "max_time = 25\n",
    "batch_size = 200\n",
    "learning_rate = 0.01\n",
    "num_epochs = 2\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(max_sent_length=max_sent_length,\n",
    "                    batch_size=batch_size,\n",
    "                    num_classes=2,\n",
    "                    sequence_length=sequence_length, \n",
    "                    vocab_size=word2vec_matrix.shape[0],\n",
    "                    embedding_size=word2vec_matrix.shape[1],\n",
    "                    filter_sizes=[1, 2, 3],\n",
    "                    conv_output_len=50,\n",
    "                    num_rnn_units=128)\n",
    "\n",
    "TF_SAVEDIR = \"./model/temp\"\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"model\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"model_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(CNNRNN)\n",
    "\n",
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "\n",
    "lm = CNNRNN.CNNRNN(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer, feed_dict={lm.embedding_: word2vec_matrix})\n",
    "    \n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        bi = batch_generator(train_x, train_y, batch_size)\n",
    "        \n",
    "        print(\"[epoch {:d}] Starting epoch {:d}\".format(epoch, epoch))\n",
    "\n",
    "        # Run a training epoch.\n",
    "        run_epoch(lm, session, bi, train=True, verbose=True, tick_s=10, learning_rate=learning_rate)\n",
    "        \n",
    "        print(\"[epoch {:d}] Completed in {:s}\".format(epoch, pretty_timedelta(since=t0_epoch)))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/temp/model_trained\n",
      "[batch 6]: loss = 0.640, accuracy = 0.649\n",
      "[batch 14]: loss = 0.645, accuracy = 0.645\n",
      "[batch 22]: loss = 0.646, accuracy = 0.650\n",
      "[batch 30]: loss = 0.648, accuracy = 0.651\n",
      "[batch 38]: loss = 0.651, accuracy = 0.648\n",
      "[batch 45]: loss = 0.651, accuracy = 0.648\n",
      "[batch 53]: loss = 0.652, accuracy = 0.648\n",
      "[batch 61]: loss = 0.652, accuracy = 0.649\n",
      "[batch 69]: loss = 0.650, accuracy = 0.652\n",
      "[batch 77]: loss = 0.646, accuracy = 0.653\n",
      "Train: avg. loss: 0.643  (perplexity: 1.90), accuracy: 0.656\n",
      "\n",
      "[batch 6]: loss = 0.652, accuracy = 0.663\n",
      "Dev: avg. loss: 0.656  (perplexity: 1.93), accuracy: 0.657\n",
      "\n",
      "[batch 6]: loss = 0.668, accuracy = 0.640\n",
      "Test: avg. loss: 0.664  (perplexity: 1.94), accuracy: 0.647\n"
     ]
    }
   ],
   "source": [
    "lm = CNNRNN.CNNRNN(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "\n",
    "with lm.graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    \n",
    "    # Load the trained model\n",
    "    saver.restore(session, trained_filename)\n",
    "\n",
    "    score_dataset(lm, session, train_x, train_y, batch_size=200, name=\"Train\")\n",
    "    print()\n",
    "    score_dataset(lm, session, dev_x, dev_y, batch_size=200, name=\"Dev\")\n",
    "    print()\n",
    "    score_dataset(lm, session, test_x, test_y, batch_size=200, name=\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/temp/model_trained\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "model_params = dict(max_sent_length=max_sent_length,\n",
    "                    batch_size=1,\n",
    "                    num_classes=2,\n",
    "                    sequence_length=sequence_length, \n",
    "                    vocab_size=word2vec_matrix.shape[0],\n",
    "                    embedding_size=word2vec_matrix.shape[1],\n",
    "                    filter_sizes=[1, 2, 3],\n",
    "                    conv_output_len=50,\n",
    "                    num_rnn_units=128)\n",
    "\n",
    "lm = CNNRNN.CNNRNN(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "\n",
    "with lm.graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "preds = []\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "\n",
    "    # Load the trained model\n",
    "    saver.restore(session, trained_filename)\n",
    "\n",
    "    for i in range(len(train_x)):\n",
    "        pred = session.run(lm.logits_, feed_dict={lm.input_x_: np.reshape(train_x[i], [1, max_sent_length, sequence_length])})\n",
    "        preds.append(np.exp(pred[0][1])/sum(np.exp(pred[0])))\n",
    "        if i % 100 == 0:\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cross_entropy_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(1791.774, 1814.6]</th>\n",
       "      <td>1.061668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1814.6, 1837.2]</th>\n",
       "      <td>1.015342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1837.2, 1859.8]</th>\n",
       "      <td>1.049687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1859.8, 1882.4]</th>\n",
       "      <td>1.074655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1882.4, 1905.0]</th>\n",
       "      <td>1.050141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1905.0, 1927.6]</th>\n",
       "      <td>1.068226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1927.6, 1950.2]</th>\n",
       "      <td>1.005111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1950.2, 1972.8]</th>\n",
       "      <td>0.917637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1972.8, 1995.4]</th>\n",
       "      <td>0.821081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1995.4, 2018.0]</th>\n",
       "      <td>0.845767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    cross_entropy_loss\n",
       "(1791.774, 1814.6]            1.061668\n",
       "(1814.6, 1837.2]              1.015342\n",
       "(1837.2, 1859.8]              1.049687\n",
       "(1859.8, 1882.4]              1.074655\n",
       "(1882.4, 1905.0]              1.050141\n",
       "(1905.0, 1927.6]              1.068226\n",
       "(1927.6, 1950.2]              1.005111\n",
       "(1950.2, 1972.8]              0.917637\n",
       "(1972.8, 1995.4]              0.821081\n",
       "(1995.4, 2018.0]              0.845767"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, _ = pd.cut([int(x) for x in train_data.year_filed], 10, retbins=True)\n",
    "bins = pd.cut([int(x) for x in train_data.year_filed], 10, labels=False)\n",
    "\n",
    "preds_df = pd.DataFrame({'bins': bins, 'log_prob': np.log(preds)})\n",
    "\n",
    "agg_df = preds_df.groupby('bins').agg(lambda x: -np.mean(x.log_prob))\n",
    "agg_df.index = out.categories\n",
    "agg_df.columns = ['cross_entropy_loss']\n",
    "\n",
    "agg_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
